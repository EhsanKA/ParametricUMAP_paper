{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make a classifier network that is as similar as possible to the umap subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:13.098043Z",
     "start_time": "2020-07-27T17:43:13.046912Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:16.946618Z",
     "start_time": "2020-07-27T17:43:16.930066Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:21.097977Z",
     "start_time": "2020-07-27T17:43:17.099275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:21.139011Z",
     "start_time": "2020-07-27T17:43:21.099912Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:25.934100Z",
     "start_time": "2020-07-27T17:43:21.140359Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import umap\n",
    "import copy\n",
    "import os, tempfile\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:26.019366Z",
     "start_time": "2020-07-27T17:43:25.935424Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.load_datasets import load_CIFAR10, mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:26.814438Z",
     "start_time": "2020-07-27T17:43:26.020593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 32, 32, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_CIFAR10(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:26.871319Z",
     "start_time": "2020-07-27T17:43:26.817530Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_per_class = 3\n",
    "X_labeled, Y_labeled, Y_masked = mask_labels(X_train, Y_train, labels_per_class = labels_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build umap graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:26.920861Z",
     "start_time": "2020-07-27T17:43:26.872790Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import build_fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:50.011318Z",
     "start_time": "2020-07-27T17:43:26.923433Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neighbors = 15  # default = 15\n",
    "umap_graph = build_fuzzy_simplicial_set(\n",
    "    X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:]))),\n",
    "    y=Y_masked,\n",
    "    n_neighbors=n_neighbors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:50.137611Z",
     "start_time": "2020-07-27T17:43:50.013252Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import compute_cross_entropy, get_graph_elements\n",
    "from tfumap.semisupervised import create_edge_iterator, create_validation_iterator, create_classification_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:50.320120Z",
     "start_time": "2020-07-27T17:43:50.138937Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "            umap_graph, n_epochs\n",
    ")\n",
    "batch_size = np.min([n_vertices, 32])\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:51.145613Z",
     "start_time": "2020-07-27T17:43:50.321447Z"
    }
   },
   "outputs": [],
   "source": [
    "labeled_iter = create_classification_iterator(X_labeled, Y_labeled, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:54.457134Z",
     "start_time": "2020-07-27T17:43:51.147280Z"
    }
   },
   "outputs": [],
   "source": [
    "max_sample_repeats_per_epoch = 25\n",
    "edge_iter, n_edges_per_epoch = create_edge_iterator(\n",
    "    head,\n",
    "    tail,\n",
    "    weight,\n",
    "    batch_size=batch_size,\n",
    "    max_sample_repeats_per_epoch=max_sample_repeats_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:54.608968Z",
     "start_time": "2020-07-27T17:43:54.459347Z"
    }
   },
   "outputs": [],
   "source": [
    "data_valid, n_valid_samp = create_validation_iterator(X_valid, Y_valid, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:54.660134Z",
     "start_time": "2020-07-27T17:43:54.610442Z"
    }
   },
   "outputs": [],
   "source": [
    "dims = (32,32,3)\n",
    "num_classes = 2\n",
    "PROJECTION_DIMS = 2\n",
    "last_layer_dims = 512\n",
    "lr_alpha = 0.1\n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:54.912630Z",
     "start_time": "2020-07-27T17:43:54.661471Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow.keras import layers\n",
    "def conv_block(filts, name, kernel_size = (3, 3), padding = \"same\", **kwargs):\n",
    "    return WeightNormalization(\n",
    "        layers.Conv2D(\n",
    "            filts, kernel_size, activation=None, padding=padding, **kwargs\n",
    "        ),\n",
    "        name=\"conv\"+name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.436446Z",
     "start_time": "2020-07-27T17:43:54.914402Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.Sequential()\n",
    "encoder.add(tf.keras.Input(shape=dims))\n",
    "### conv1a\n",
    "name = '1a'\n",
    "encoder.add(conv_block(name = name, filts = 128, kernel_size = (3,3), padding=\"same\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "### conv1b\n",
    "name = '1b'\n",
    "encoder.add(conv_block(name = name, filts = 128, kernel_size = (3,3), padding=\"same\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "### conv1c\n",
    "name = '1c'\n",
    "encoder.add(conv_block(name = name, filts = 128, kernel_size = (3,3), padding=\"same\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "# max pooling\n",
    "encoder.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', name=\"mp1\"))\n",
    "# dropout\n",
    "encoder.add(layers.Dropout(dropout_rate, name=\"drop1\"))\n",
    "\n",
    "### conv2a\n",
    "name = '2a'\n",
    "encoder.add(conv_block(name = name, filts = 256, kernel_size = (3,3), padding=\"same\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha))\n",
    "\n",
    "### conv2b\n",
    "name = '2b'\n",
    "encoder.add(conv_block(name = name, filts = 256, kernel_size = (3,3), padding=\"same\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "### conv2c\n",
    "name = '2c'\n",
    "encoder.add(conv_block(name = name, filts = 256, kernel_size = (3,3), padding=\"same\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "# max pooling\n",
    "encoder.add(layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', name=\"mp2\"))\n",
    "# dropout\n",
    "encoder.add(layers.Dropout(dropout_rate, name=\"drop2\"))\n",
    "\n",
    "### conv3a\n",
    "name = '3a'\n",
    "encoder.add(conv_block(name = name, filts = 512, kernel_size = (3,3), padding=\"valid\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "### conv3b\n",
    "name = '3b'\n",
    "encoder.add(conv_block(name = name, filts = 256, kernel_size = (1,1), padding=\"valid\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "### conv3c\n",
    "name = '3c'\n",
    "encoder.add(conv_block(name = name, filts = 128, kernel_size = (1,1), padding=\"valid\"))\n",
    "encoder.add(layers.BatchNormalization(name=\"bn\"+name))\n",
    "encoder.add(layers.LeakyReLU(alpha=lr_alpha, name = 'lrelu'+name))\n",
    "\n",
    "# max pooling\n",
    "encoder.add(layers.AveragePooling2D(pool_size=(6, 6), strides=2, padding='valid'))\n",
    "encoder.add(layers.Flatten())\n",
    "encoder.add(layers.Dense(last_layer_dims, activation=None, name='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.628317Z",
     "start_time": "2020-07-27T17:43:57.437907Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential()\n",
    "classifier.add(tf.keras.Input(shape=(last_layer_dims)))\n",
    "encoder.add(layers.Dense(512, activation='relu'))\n",
    "classifier.add(layers.Dense(num_classes, activation=None, name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.715662Z",
     "start_time": "2020-07-27T17:43:57.629881Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = tf.keras.Sequential()\n",
    "embedder.add(tf.keras.Input(shape=(last_layer_dims)))\n",
    "encoder.add(layers.Dense(512, activation='relu'))\n",
    "embedder.add(layers.Dense(PROJECTION_DIMS, activation=None, name='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.782098Z",
     "start_time": "2020-07-27T17:43:57.717100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1a (WeightNormalization) (None, 32, 32, 128)       7297      \n",
      "_________________________________________________________________\n",
      "bn1a (BatchNormalization)    (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "lrelu1a (LeakyReLU)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv1b (WeightNormalization) (None, 32, 32, 128)       295297    \n",
      "_________________________________________________________________\n",
      "bn1b (BatchNormalization)    (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "lrelu1b (LeakyReLU)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv1c (WeightNormalization) (None, 32, 32, 128)       295297    \n",
      "_________________________________________________________________\n",
      "bn1c (BatchNormalization)    (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "lrelu1c (LeakyReLU)          (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "mp1 (MaxPooling2D)           (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "drop1 (Dropout)              (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2a (WeightNormalization) (None, 16, 16, 256)       590593    \n",
      "_________________________________________________________________\n",
      "bn2a (BatchNormalization)    (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2b (WeightNormalization) (None, 16, 16, 256)       1180417   \n",
      "_________________________________________________________________\n",
      "bn2b (BatchNormalization)    (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "lrelu2b (LeakyReLU)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2c (WeightNormalization) (None, 16, 16, 256)       1180417   \n",
      "_________________________________________________________________\n",
      "bn2c (BatchNormalization)    (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "lrelu2c (LeakyReLU)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "mp2 (MaxPooling2D)           (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "drop2 (Dropout)              (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv3a (WeightNormalization) (None, 6, 6, 512)         2360833   \n",
      "_________________________________________________________________\n",
      "bn3a (BatchNormalization)    (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "lrelu3a (LeakyReLU)          (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv3b (WeightNormalization) (None, 6, 6, 256)         262913    \n",
      "_________________________________________________________________\n",
      "bn3b (BatchNormalization)    (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "lrelu3b (LeakyReLU)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv3c (WeightNormalization) (None, 6, 6, 128)         65921     \n",
      "_________________________________________________________________\n",
      "bn3c (BatchNormalization)    (None, 6, 6, 128)         512       \n",
      "_________________________________________________________________\n",
      "lrelu3c (LeakyReLU)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 1, 1, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "z (Dense)                    (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               262656    \n",
      "=================================================================\n",
      "Total params: 6,838,537\n",
      "Trainable params: 3,715,968\n",
      "Non-trainable params: 3,122,569\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create UMAP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.828314Z",
     "start_time": "2020-07-27T17:43:57.783644Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import compute_cross_entropy, convert_distance_to_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.870521Z",
     "start_time": "2020-07-27T17:43:57.829295Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import find_a_b, compute_umap_loss, compute_classifier_loss, batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:57.926451Z",
     "start_time": "2020-07-27T17:43:57.871805Z"
    }
   },
   "outputs": [],
   "source": [
    "class PUMAP(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        embedder,\n",
    "        classifier,\n",
    "        tensorboard_logdir=None,  # directory for tensorboard log\n",
    "        min_dist=0.1,\n",
    "        negative_sample_rate=5.0,\n",
    "        optimizer=tf.keras.optimizers.SGD(0.1),\n",
    "        repulsion_strength=1.0,\n",
    "        umap_prop=1.0, # to what extent do we train UMAP\n",
    "        # ** kwargs,\n",
    "    ):\n",
    "        super(PUMAP, self).__init__()\n",
    "        # self.__dict__.update(kwargs)\n",
    "\n",
    "        # subnetworks\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "        self.umap_prop = umap_prop\n",
    "\n",
    "        # optimizer for cross entropy minimization\n",
    "        self.optimizer = optimizer\n",
    "        self.classifier_optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "\n",
    "        # get a,b for current min_dist\n",
    "        self._a, self._b = find_a_b(min_dist)\n",
    "\n",
    "        # create summary writer to log loss information during training\n",
    "        if tensorboard_logdir is None:\n",
    "            self.tensorboard_logdir = os.path.join(\n",
    "                tempfile.gettempdir(),\n",
    "                \"tensorboard\",\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            )\n",
    "        else:\n",
    "            self.tensorboard_logdir = tensorboard_logdir\n",
    "        self.summary_writer_train = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/train\"\n",
    "        )\n",
    "        self.summary_writer_valid = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/valid\"\n",
    "        )\n",
    "\n",
    "        # sparse categorical cross entropy\n",
    "        self.sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "        self.create_summary_metrics()\n",
    "\n",
    "    def create_summary_metrics(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Create keras summary objects for loss\n",
    "        \"\"\"\n",
    "        self.summary_metrics = {}\n",
    "        self.summary_metrics[\"train_loss_umap\"] = tf.keras.metrics.Mean(\n",
    "            \"train_loss_umap\", dtype=tf.float32\n",
    "        )\n",
    "        self.summary_metrics[\"train_loss_classif\"] = tf.keras.metrics.Mean(\n",
    "            \"train_loss_classif\", dtype=tf.float32\n",
    "        )\n",
    "        self.summary_metrics[\"valid_loss_classif\"] = tf.keras.metrics.Mean(\n",
    "            \"valid_loss_classif\", dtype=tf.float32\n",
    "        )\n",
    "        self.summary_metrics[\"train_acc_classif\"] = tf.keras.metrics.Accuracy(\n",
    "            \"train_acc_classif\", dtype=tf.float32\n",
    "        )\n",
    "        self.summary_metrics[\"valid_acc_classif\"] = tf.keras.metrics.Accuracy(\n",
    "            \"valid_acc_classif\", dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    def write_acc(self, X, y, batch, train=True):\n",
    "        predictions = self.classify_data(X)\n",
    "\n",
    "        if train:\n",
    "            summary_writer = self.summary_writer_train\n",
    "            train_valid = \"train\"\n",
    "        else:\n",
    "            summary_writer = self.summary_writer_valid\n",
    "            train_valid = \"valid\"\n",
    "\n",
    "        self.summary_metrics[train_valid + \"_acc_classif\"].update_state(predictions, y)\n",
    "\n",
    "        # write loss\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"acc_classif\",\n",
    "                self.summary_metrics[train_valid + \"_acc_classif\"].result(),\n",
    "                step=batch,\n",
    "            )\n",
    "            summary_writer.flush()\n",
    "\n",
    "    def write_losses(self, batch, classif_loss, umap_loss=None, train=True):\n",
    "        # add umap loss\n",
    "        if train:\n",
    "            summary_writer = self.summary_writer_train\n",
    "            train_valid = \"train\"\n",
    "            self.summary_metrics[train_valid + \"_loss_umap\"](umap_loss)\n",
    "        else:\n",
    "            summary_writer = self.summary_writer_valid\n",
    "            train_valid = \"valid\"\n",
    "\n",
    "        # add valid loss\n",
    "        self.summary_metrics[train_valid + \"_loss_classif\"](classif_loss)\n",
    "\n",
    "        # write loss\n",
    "        with summary_writer.as_default():\n",
    "            if train:\n",
    "                tf.summary.scalar(\n",
    "                    \"umap_loss\",\n",
    "                    self.summary_metrics[train_valid + \"_loss_umap\"].result(),\n",
    "                    step=batch,\n",
    "                )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\",\n",
    "                self.summary_metrics[train_valid + \"_loss_classif\"].result(),\n",
    "                step=batch,\n",
    "            )\n",
    "            summary_writer.flush()\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, batch_to, batch_from, X, y, save_loss=False):\n",
    "        \"\"\" One training step \n",
    "        Input are points and weights for positive and negative \n",
    "        samples for training. \n",
    "            \n",
    "        \"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            (attraction_loss, repellant_loss, umap_loss) = compute_umap_loss(\n",
    "                batch_to,\n",
    "                batch_from,\n",
    "                self.embedder,\n",
    "                self.encoder,\n",
    "                self._a,\n",
    "                self._b,\n",
    "                self.negative_sample_rate,\n",
    "                self.repulsion_strength,\n",
    "            )\n",
    "\n",
    "            classifier_loss = compute_classifier_loss(\n",
    "                X, y, self.encoder, self.classifier, self.sparse_ce\n",
    "            )\n",
    "            loss = tf.reduce_sum(classifier_loss)  + tf.reduce_sum(umap_loss)*self.umap_prop\n",
    "\n",
    "        # compute gradient for umap\n",
    "        grad = tape.gradient(\n",
    "            loss,\n",
    "            self.encoder.trainable_variables\n",
    "            + self.embedder.trainable_variables\n",
    "            + self.classifier.trainable_variables,\n",
    "        )\n",
    "\n",
    "        # gradients are cliped in UMAP implementation. Any effect here?\n",
    "        grad = [tf.clip_by_value(grad, -4.0, 4.0) for grad in grad]\n",
    "\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(\n",
    "                grad,\n",
    "                self.encoder.trainable_variables\n",
    "                + self.embedder.trainable_variables\n",
    "                + self.classifier.trainable_variables,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return (attraction_loss, repellant_loss, umap_loss, classifier_loss)\n",
    "\n",
    "    # @tf.function\n",
    "    def project_epoch(self, X):\n",
    "        \"\"\" Train a batch in tensorflow\n",
    "        \"\"\"\n",
    "        return [self.embedder(self.encoder(batch)) for batch in X]\n",
    "\n",
    "    def get_dataset_loss(self, dataset):\n",
    "        return [\n",
    "            compute_classifier_loss(X, y, self.encoder, self.classifier, self.sparse_ce)\n",
    "            for X, y in dataset\n",
    "        ]\n",
    "\n",
    "    def classify_data(self, X):\n",
    "        \"\"\" Classify a set of points X\n",
    "        \"\"\"\n",
    "        X_batch = batch_data(X, batch_size=100)\n",
    "        predictions = tf.concat([classifier(encoder(i)) for i in tqdm(X_batch)], axis=0)\n",
    "        predictions = tf.nn.softmax(predictions) \n",
    "        return tf.argmax(predictions, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:43:58.030941Z",
     "start_time": "2020-07-27T17:43:57.928545Z"
    }
   },
   "outputs": [],
   "source": [
    "### Initialize model\n",
    "model = PUMAP(\n",
    "    min_dist = 0.0,\n",
    "    negative_sample_rate = 5, # how many negative samples per positive\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-2), # cross-entropy optimizer\n",
    "    encoder=encoder,\n",
    "    embedder=embedder,\n",
    "    classifier=classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:44:12.177065Z",
     "start_time": "2020-07-27T17:44:11.902776Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_plotting import plot_umap_classif_results, plot_results, get_decision_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-27T17:44:13.898Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No objects to concatenate\n"
     ]
    }
   ],
   "source": [
    "plot_umap_classif_results(model, X_valid, Y_valid, X_train, X_labeled, Y_labeled, batch_size, cmap='tab10', cmap2='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-27T17:44:14.946Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = 0; epoch = 0\n",
    "N_EPOCHS = 50 # total number of epochs\n",
    "SAMPLE_EPOCHS = np.linspace(0, N_EPOCHS-1, 10).astype('int') # how often to plot\n",
    "print(SAMPLE_EPOCHS[:10])\n",
    "BATCHES_PER_EPOCH = int(n_edges_per_epoch / batch_size)\n",
    "TOTAL_BATCHES = BATCHES_PER_EPOCH * N_EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-27T17:44:15.514Z"
    }
   },
   "outputs": [],
   "source": [
    "#z_valid = UMAP(verbose=True).fit_transform(latent_valid.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-27T17:44:22.691Z"
    }
   },
   "outputs": [],
   "source": [
    "for edge_epoch, epoch in tqdm(zip(edge_iter, np.arange(N_EPOCHS)), total=N_EPOCHS):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation loss\n",
    "    classif_loss = model.get_dataset_loss(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-27T17:23:34.498Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd674d7ac35467baebc601117d4db3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18419595b2d47c9b6fdebf3f1f61cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3515a8a516844cbe94e5ab1f0e83ec14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9da309c43eb4c069e7f6dc942552d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=337588), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for edge_epoch, epoch in tqdm(zip(edge_iter, np.arange(N_EPOCHS)), total=N_EPOCHS):\n",
    "    \n",
    "    # validation loss\n",
    "    classif_loss = model.get_dataset_loss(data_valid)\n",
    "    model.write_losses(\n",
    "            batch=batch, classif_loss=classif_loss, umap_loss=None, train=False\n",
    "        )\n",
    "    model.write_acc(X_valid, Y_valid, batch, train=False)\n",
    "    model.write_acc(X_labeled, Y_labeled, batch, train=True)\n",
    "    \n",
    "    # train\n",
    "    for (batch_to, batch_from), (X, y) in tqdm(\n",
    "        zip(edge_epoch, labeled_iter), total=BATCHES_PER_EPOCH, leave=False\n",
    "    ):\n",
    "        batch_to = X_train[batch_to]\n",
    "        batch_from = X_train[batch_from]\n",
    "        # train model\n",
    "        losses = model.train(batch_to=batch_to, batch_from=batch_from, X=X, y=y)\n",
    "        model.write_losses(\n",
    "            batch=batch, classif_loss=losses[3], umap_loss=losses[2], train=True\n",
    "        )\n",
    "    # plot\n",
    "    if epoch in SAMPLE_EPOCHS:\n",
    "        print(\"batch: {}\".format(batch))\n",
    "        plot_umap_classif_results(model, X_valid, Y_valid, X_train, X_labeled, Y_labeled, batch_size)\n",
    "    batch += BATCHES_PER_EPOCH\n",
    "    # batch_tqdm.update(BATCHES_PER_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-27T17:23:34.499Z"
    }
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-27T17:21:02.085141Z",
     "start_time": "2020-07-27T17:20:58.447086Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:17.187399Z",
     "start_time": "2020-08-08T16:43:17.155856Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:17.269447Z",
     "start_time": "2020-08-08T16:43:17.189466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:25.953510Z",
     "start_time": "2020-08-08T16:43:17.272991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:26.017528Z",
     "start_time": "2020-08-08T16:43:25.956053Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:26.117372Z",
     "start_time": "2020-08-08T16:43:26.020878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar10_1.0_1024_1024____2020_08_08_09_43_26_114596__learned-metric_augmented\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dataset = \"cifar10\"\n",
    "dims = (32, 32, 3)\n",
    "umap_prop = 1.0\n",
    "num_classes = 10\n",
    "PROJECTION_DIMS = 1024\n",
    "labels_per_class = 1024  #'full'\n",
    "datestring = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "datestring = (\n",
    "    str(dataset)\n",
    "    + \"_\"\n",
    "    + str(umap_prop)\n",
    "    + \"_\"\n",
    "    + str(PROJECTION_DIMS)\n",
    "    + \"_\"\n",
    "    + str(labels_per_class)\n",
    "    + \"____\"\n",
    "    + datestring\n",
    "    + \"__learned-metric_augmented\"\n",
    ")\n",
    "print(datestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:34.392949Z",
     "start_time": "2020-08-08T16:43:26.119890Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import umap\n",
    "import copy\n",
    "import os, tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:34.724671Z",
     "start_time": "2020-08-08T16:43:34.397231Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.load_datasets import load_CIFAR10, mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:37.274204Z",
     "start_time": "2020-08-08T16:43:34.728294Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_CIFAR10(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:37.441541Z",
     "start_time": "2020-08-08T16:43:37.277354Z"
    }
   },
   "outputs": [],
   "source": [
    "if labels_per_class == \"full\":\n",
    "    X_labeled = X_train\n",
    "    Y_masked = Y_labeled = Y_train\n",
    "else:\n",
    "    X_labeled, Y_labeled, Y_masked = mask_labels(\n",
    "        X_train, Y_train, labels_per_class=labels_per_class\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:37.617882Z",
     "start_time": "2020-08-08T16:43:37.443520Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import compute_cross_entropy, get_graph_elements\n",
    "from tfumap.semisupervised import create_edge_iterator, create_validation_iterator, create_classification_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:43:37.689738Z",
     "start_time": "2020-08-08T16:43:37.619936Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import build_fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:10.654858Z",
     "start_time": "2020-08-08T16:43:37.691465Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neighbors = 15  # default = 15\n",
    "umap_graph = build_fuzzy_simplicial_set(\n",
    "    X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:]))),\n",
    "    y=Y_masked,\n",
    "    n_neighbors=n_neighbors,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:10.900991Z",
     "start_time": "2020-08-08T16:44:10.659798Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "            umap_graph, n_epochs\n",
    ")\n",
    "batch_size = 128\n",
    "#batch_size = np.min([n_vertices, 1000])\n",
    "#batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:12.309989Z",
     "start_time": "2020-08-08T16:44:10.904264Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# make sure batch size is no bigger than the number of labels per class\n",
    "labeled_batch_size = batch_size if batch_size < len(Y_labeled) else len(Y_labeled)\n",
    "labeled_iter = create_classification_iterator(X_labeled, Y_labeled, batch_size=batch_size)\n",
    "print(labeled_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:20.724443Z",
     "start_time": "2020-08-08T16:44:12.313078Z"
    }
   },
   "outputs": [],
   "source": [
    "max_sample_repeats_per_epoch = 25\n",
    "edge_iter, n_edges_per_epoch = create_edge_iterator(\n",
    "    head,\n",
    "    tail,\n",
    "    weight,\n",
    "    batch_size=batch_size,\n",
    "    max_sample_repeats_per_epoch=max_sample_repeats_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:21.104161Z",
     "start_time": "2020-08-08T16:44:20.729505Z"
    }
   },
   "outputs": [],
   "source": [
    "data_valid, n_valid_samp = create_validation_iterator(X_valid, Y_valid, batch_size, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:22.467957Z",
     "start_time": "2020-08-08T16:44:21.107677Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "block_layers_num = 3\n",
    "weight_decay = 1e-4\n",
    "name = 'resnet20'\n",
    "def conv2d_bn(x, filters, kernel_size, weight_decay=.0, strides=(1, 1)):\n",
    "    layer = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same',\n",
    "                   use_bias=False,\n",
    "                   kernel_regularizer=l2(weight_decay)\n",
    "                   )(x)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv2d_bn_relu(x, filters, kernel_size, weight_decay=.0, strides=(1, 1)):\n",
    "    layer = conv2d_bn(x, filters, kernel_size, weight_decay, strides)\n",
    "    layer = Activation('relu')(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def ResidualBlock(x, filters, kernel_size, weight_decay, downsample=True):\n",
    "    if downsample:\n",
    "        # residual_x = conv2d_bn_relu(x, filters, kernel_size=1, strides=2)\n",
    "        residual_x = conv2d_bn(x, filters, kernel_size=1, strides=2)\n",
    "        stride = 2\n",
    "    else:\n",
    "        residual_x = x\n",
    "        stride = 1\n",
    "    residual = conv2d_bn_relu(x,\n",
    "                              filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              weight_decay=weight_decay,\n",
    "                              strides=stride,\n",
    "                              )\n",
    "    residual = conv2d_bn(residual,\n",
    "                         filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         weight_decay=weight_decay,\n",
    "                         strides=1,\n",
    "                         )\n",
    "    out = layers.add([residual_x, residual])\n",
    "    out = Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "input_ = Input(shape=dims)\n",
    "x =input_\n",
    "x = conv2d_bn_relu(x, filters=16, kernel_size=(3, 3), weight_decay=weight_decay, strides=(1, 1))\n",
    "\n",
    "# # conv 2\n",
    "for i in range(block_layers_num):\n",
    "    x = ResidualBlock(x, filters=16, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "# # conv 3\n",
    "x = ResidualBlock(x, filters=32, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "for i in range(block_layers_num - 1):\n",
    "    x = ResidualBlock(x, filters=32, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "# # conv 4\n",
    "x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "for i in range(block_layers_num - 1):\n",
    "    x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "x = AveragePooling2D(pool_size=(8, 8), padding='valid')(x)\n",
    "x = Flatten()(x)\n",
    "encoder = Model(input_, x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:22.674758Z",
     "start_time": "2020-08-08T16:44:22.470411Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential()\n",
    "classifier.add(tf.keras.layers.InputLayer(input_shape=64))\n",
    "classifier.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "classifier.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "classifier.add(tf.keras.layers.Dense(num_classes, activation='softmax', name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:22.814247Z",
     "start_time": "2020-08-08T16:44:22.677770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   432         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2304        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2304        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           activation[0][0]                 \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 16)   0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2304        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 16)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2304        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           activation_2[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 16)   0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2304        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2304        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 32, 16)   0           activation_4[0][0]               \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 16)   0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   4608        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 32)   128         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 16, 16, 32)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 32)   512         activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 32)   9216        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 32)   128         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 32)   128         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 16, 16, 32)   0           batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 16, 16, 32)   0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 32)   9216        activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 32)   128         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 16, 16, 32)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 32)   9216        activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 32)   128         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 32)   0           activation_8[0][0]               \n",
      "                                                                 batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 16, 16, 32)   0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 16, 16, 32)   9216        activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 16, 16, 32)   128         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 16, 16, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 16, 16, 32)   9216        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 16, 16, 32)   128         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 16, 16, 32)   0           activation_10[0][0]              \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 16, 16, 32)   0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 64)     18432       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 64)     256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8, 8, 64)     0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 64)     2048        activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 64)     36864       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 64)     256         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 64)     256         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 64)     0           batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8, 8, 64)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 64)     36864       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 64)     256         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 8, 8, 64)     0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 64)     36864       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 64)     256         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 64)     0           activation_14[0][0]              \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 8, 8, 64)     36864       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 8, 8, 64)     256         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 64)     0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 8, 8, 64)     36864       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 8, 8, 64)     256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 64)     0           activation_16[0][0]              \n",
      "                                                                 batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 64)     0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 1, 1, 64)     0           activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 64)           0           average_pooling2d[0][0]          \n",
      "==================================================================================================\n",
      "Total params: 273,392\n",
      "Trainable params: 271,824\n",
      "Non-trainable params: 1,568\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create UMAP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:22.887199Z",
     "start_time": "2020-08-08T16:44:22.816188Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_model import PUMAP, compute_classifier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:22.969952Z",
     "start_time": "2020-08-08T16:44:22.888977Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_umap_loss(\n",
    "    batch_to,\n",
    "    batch_from,\n",
    "    encoder,\n",
    "    _a,\n",
    "    _b,\n",
    "    negative_sample_rate=5,\n",
    "    repulsion_strength=1,\n",
    "):\n",
    "    \"\"\"\n",
    "        compute the cross entropy loss for learning embeddings\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_to : tf.int or tf.float32\n",
    "            Either X or the index locations of the embeddings for verticies (to)\n",
    "        batch_from : tf.int or tf.float32\n",
    "            Either X or the index locations of the embeddings for verticies (from)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ce_loss : tf.float\n",
    "            cross entropy loss for UMAP\n",
    "        embedding_to : tf.float\n",
    "            embeddings for verticies (to)\n",
    "        embedding_from : tf.float\n",
    "            embeddings for verticies (from)\n",
    "        \"\"\"\n",
    "\n",
    "    # encode\n",
    "    embedding_to = encoder(batch_to)\n",
    "    embedding_from = encoder(batch_from)\n",
    "\n",
    "    # get negative samples\n",
    "    embedding_neg_to = tf.repeat(embedding_to, negative_sample_rate, axis=0)\n",
    "    repeat_neg = tf.repeat(embedding_from, negative_sample_rate, axis=0)\n",
    "    embedding_neg_from = tf.gather(\n",
    "        repeat_neg, tf.random.shuffle(tf.range(tf.shape(repeat_neg)[0]))\n",
    "    )\n",
    "\n",
    "    #  distances between samples\n",
    "    distance_embedding = tf.concat(\n",
    "        [\n",
    "            tf.norm(embedding_to - embedding_from, axis=1),\n",
    "            tf.norm(embedding_neg_to - embedding_neg_from, axis=1),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # convert probabilities to distances\n",
    "    probabilities_distance = convert_distance_to_probability(distance_embedding, _a, _b)\n",
    "\n",
    "    # treat positive samples as p=1, and negative samples as p=0\n",
    "    probabilities_graph = tf.concat(\n",
    "        [tf.ones(embedding_to.shape[0]), tf.zeros(embedding_neg_to.shape[0])], axis=0,\n",
    "    )\n",
    "\n",
    "    # cross entropy loss\n",
    "    (attraction_loss, repellant_loss, ce_loss) = compute_cross_entropy(\n",
    "        probabilities_graph,\n",
    "        probabilities_distance,\n",
    "        repulsion_strength=repulsion_strength,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        attraction_loss,\n",
    "        repellant_loss,\n",
    "        ce_loss,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:23.101259Z",
     "start_time": "2020-08-08T16:44:22.971821Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfumap.umap import compute_cross_entropy, convert_distance_to_probability\n",
    "from tfumap.semisupervised import (\n",
    "    find_a_b,\n",
    "    #compute_umap_loss,\n",
    "    # compute_classifier_loss,\n",
    "    batch_data,\n",
    ")\n",
    "import numpy as np\n",
    "import os, tempfile\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_classifier_loss(X, y, encoder, classifier, sparse_ce, acc_func):\n",
    "    \"\"\" compute the cross entropy loss for classification\n",
    "        \"\"\"\n",
    "    d = classifier(encoder(X))\n",
    "    loss = sparse_ce(y, d)\n",
    "    acc = acc_func(y, d)\n",
    "    #acc = tf.keras.metrics.sparse_categorical_accuracy(y, d)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "class PUMAP(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        classifier,\n",
    "        tensorboard_logdir=None,  # directory for tensorboard log\n",
    "        min_dist=0.1,\n",
    "        negative_sample_rate=5.0,\n",
    "        optimizer=tf.keras.optimizers.SGD(0.1),\n",
    "        repulsion_strength=1.0,\n",
    "        # ** kwargs,\n",
    "    ):\n",
    "        super(PUMAP, self).__init__()\n",
    "        # self.__dict__.update(kwargs)\n",
    "\n",
    "        # subnetworks\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "\n",
    "        # optimizer for cross entropy minimization\n",
    "        self.optimizer = optimizer\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "\n",
    "        # get a,b for current min_dist\n",
    "        self._a, self._b = find_a_b(min_dist)\n",
    "\n",
    "        # create summary writer to log loss information during training\n",
    "        if tensorboard_logdir is None:\n",
    "            self.tensorboard_logdir = os.path.join(\n",
    "                tempfile.gettempdir(),\n",
    "                \"tensorboard\",\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            )\n",
    "        else:\n",
    "            self.tensorboard_logdir = tensorboard_logdir\n",
    "        self.summary_writer_train = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/train\"\n",
    "        )\n",
    "        self.summary_writer_valid = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/valid\"\n",
    "        )\n",
    "\n",
    "        # sparse categorical cross entropy\n",
    "        self.sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.class_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.class_acc_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        # self.create_summary_metrics()\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, batch_to, batch_from, X, y, umap_prop = 1.0, save_loss=False):\n",
    "        \"\"\" One training step \n",
    "        Input are points and weights for positive and negative \n",
    "        samples for training. \n",
    "            \n",
    "        \"\"\"\n",
    "        if umap_prop > 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                (attraction_loss, repellant_loss, umap_loss) = compute_umap_loss(\n",
    "                    batch_to,\n",
    "                    batch_from,\n",
    "                    self.encoder,\n",
    "                    self._a,\n",
    "                    self._b,\n",
    "                    self.negative_sample_rate,\n",
    "                    self.repulsion_strength,\n",
    "                )\n",
    "            \n",
    "                classifier_loss, classifier_acc = compute_classifier_loss(\n",
    "                    X, y, self.encoder, self.classifier, self.sparse_ce, self.class_acc\n",
    "                )\n",
    "                loss = (\n",
    "                    tf.reduce_mean(classifier_loss)\n",
    "                    + tf.reduce_mean(umap_loss) * umap_prop\n",
    "                )\n",
    "\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables\n",
    "                + self.classifier.trainable_variables\n",
    "            )\n",
    "        else:  # ignore running costly UMAP computations if not training on UMAP loss\n",
    "            with tf.GradientTape() as tape:\n",
    "                classifier_loss, classifier_acc = compute_classifier_loss(\n",
    "                    X, y, self.encoder, self.classifier, self.sparse_ce, self.class_acc\n",
    "                )\n",
    "                loss = classifier_loss\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.classifier.trainable_variables\n",
    "            )\n",
    "            attraction_loss = repellant_loss = umap_loss = 0\n",
    "\n",
    "        # compute gradient for umap\n",
    "        grad = tape.gradient(loss, train_vars)\n",
    "\n",
    "        # gradients are cliped in UMAP implementation. Any effect here?\n",
    "        grad = [tf.clip_by_value(grad, -4.0, 4.0) for grad in grad]\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(grad, train_vars))\n",
    "\n",
    "        return (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            tf.reduce_mean(umap_loss),\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def write_losses(\n",
    "        self,\n",
    "        step,\n",
    "        classifier_acc,\n",
    "        classifier_loss,\n",
    "        umap_loss,\n",
    "        classifier_loss_val,\n",
    "        classifier_acc_val,\n",
    "    ):\n",
    "        # write train loss\n",
    "        with self.summary_writer_train.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"classif_acc\", classifier_acc, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\", classifier_loss, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"umap_loss\", umap_loss, step=step,\n",
    "            )\n",
    "            self.summary_writer_train.flush()\n",
    "        # write valid loss\n",
    "        with self.summary_writer_valid.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"classif_acc\", classifier_acc_val, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\", classifier_loss_val, step=step,\n",
    "            )\n",
    "            self.summary_writer_valid.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:23.335301Z",
     "start_time": "2020-08-08T16:44:23.110422Z"
    }
   },
   "outputs": [],
   "source": [
    "### Initialize model\n",
    "model = PUMAP(\n",
    "    min_dist = 0.0,\n",
    "    negative_sample_rate = 5, # how many negative samples per positive\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3), # cross-entropy optimizer\n",
    "    encoder=encoder,\n",
    "    classifier=classifier,    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:23.440071Z",
     "start_time": "2020-08-08T16:44:23.339912Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_data(X, batch_size, model):\n",
    "    \"\"\" embed a set of points in X to Z\n",
    "    \"\"\"\n",
    "    n_batch = int(np.ceil(len(X) / batch_size))\n",
    "    return np.vstack(\n",
    "        [\n",
    "                model(np.array(X[(i) * batch_size : (i + 1) * batch_size, :]))\n",
    "            for i in tqdm(range(n_batch), leave=False)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-08T16:44:23.704548Z",
     "start_time": "2020-08-08T16:44:23.443065Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import retrieve_tensors\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from umap import UMAP \n",
    "\n",
    "def plot_umap_classif_results(\n",
    "    model,\n",
    "    X_valid,\n",
    "    Y_valid,\n",
    "    X_train,\n",
    "    X_labeled,\n",
    "    Y_labeled,\n",
    "    batch_size,\n",
    "    dralg = PCA, #umap\n",
    "    cmap=\"coolwarm\",\n",
    "    cmap2=\"bwr\",\n",
    "):\n",
    "    # get loss dataframe from tensorboard\n",
    "    try:\n",
    "        loss_df = retrieve_tensors(model.tensorboard_logdir)\n",
    "        losses_exist = True\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        losses_exist = False\n",
    "\n",
    "    pred_model = tf.keras.models.Model(model.classifier.input,[model.classifier.get_layer(name=\"dense_1\").get_output_at(0)])\n",
    "\n",
    "    # embed data\n",
    "    enc_valid = model_data(X_valid, batch_size, model.encoder)\n",
    "    class_valid = model_data(enc_valid, batch_size, pred_model)\n",
    "    pred_valid = model_data(enc_valid, batch_size, model.classifier)\n",
    "\n",
    "    enc_lab = model_data(X_labeled, batch_size, model.encoder)\n",
    "    class_lab = model_data(enc_lab, batch_size, pred_model)\n",
    "    pred_lab = model_data(enc_lab, batch_size, model.classifier)\n",
    "\n",
    "    # get classifications and correctness\n",
    "    classifications_valid = np.argmax(pred_valid, axis=1)\n",
    "    correctness_valid = classifications_valid == Y_valid\n",
    "\n",
    "    classifications_lab = np.argmax(pred_lab, axis=1)\n",
    "    correctness_lab = classifications_lab == np.array(Y_labeled)\n",
    "\n",
    "    confidence_lab  = np.max(pred_lab, axis=1)\n",
    "    confidence_valid = np.max(pred_valid, axis=1)\n",
    "\n",
    "    thresh = 0.0 #0.8\n",
    "    mask_valid = confidence_valid > thresh\n",
    "    mask_lab = confidence_lab > thresh\n",
    "\n",
    "    # embed into 2D \n",
    "    dr = dralg()\n",
    "    enc_valid_2 = dr.fit_transform(enc_valid[mask_valid])\n",
    "    enc_lab_2 = dr.transform(enc_lab[mask_lab])\n",
    "\n",
    "\n",
    "\n",
    "    dr = dralg()\n",
    "    class_valid_2 = dr.fit_transform(class_valid[mask_valid])\n",
    "    class_lab_2 = dr.transform(class_lab[mask_lab])\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(14, 6), gridspec_kw = {'wspace':0, 'hspace':0})\n",
    "    for row, (z_val, z_lab, z_title) in enumerate([\n",
    "        [enc_valid_2, enc_lab_2, 'encoder'], \n",
    "        [class_valid_2, class_lab_2, 'last_layer_classifier'], \n",
    "\n",
    "    ]):\n",
    "        for col, (y_valid, y_labeled, label_name) in enumerate([\n",
    "            [Y_valid, Y_labeled, 'ground_truth'],\n",
    "            [classifications_valid, classifications_lab, 'predictions'],\n",
    "            [correctness_valid, correctness_lab, 'correctness'],\n",
    "            [confidence_valid, confidence_lab, 'confidence'],\n",
    "        ]):\n",
    "            ax = axs[row, col]\n",
    "            #ax.axis('off')\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(z_title)\n",
    "            if row == 0:\n",
    "                ax.set_title(label_name)\n",
    "            #ax.set_title(z_title + '_' + label_name)\n",
    "            ax.scatter(\n",
    "                z_val[:, 0],\n",
    "                z_val[:, 1],\n",
    "                c=y_valid[mask_valid],\n",
    "                cmap=cmap if label_name not in [\"correctness\", \"confidence\"]  else 'viridis',  # \"tab10\"\n",
    "                s=2,\n",
    "                alpha=0.25,\n",
    "                rasterized=True,\n",
    "            )\n",
    "            if False:\n",
    "                ax.scatter(\n",
    "                    z_lab[:, 0],\n",
    "                    z_lab[:, 1],\n",
    "                    c=y_labeled[mask_lab],\n",
    "                    cmap=cmap if label_name not in [\"correctness\", \"confidence\"] else 'viridis',  # \"tab10\"\n",
    "                    s=20,\n",
    "                    alpha=0.25,\n",
    "                    rasterized=True,\n",
    "                )\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=3, figsize=(14,2))\n",
    "    if losses_exist:\n",
    "        ax = axs[0]\n",
    "        sns.lineplot(\n",
    "            x=\"step\",\n",
    "            y=\"val\",\n",
    "            hue=\"group\",\n",
    "            data=loss_df[loss_df.variable == \"umap_loss\"],\n",
    "            ci=None,\n",
    "            legend=None,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_title(\"UMAP loss\", fontsize=24)\n",
    "        ax.set_ylabel(\"Cross Entropy\")\n",
    "        \n",
    "        ax = axs[1]\n",
    "        sns.lineplot(\n",
    "            x=\"step\",\n",
    "            y=\"val\",\n",
    "            hue=\"group\",\n",
    "            data=loss_df[loss_df.variable == \"classif_loss\"],\n",
    "            ci=None,\n",
    "            legend=None,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        # ax.set_yscale(\"log\")\n",
    "        ax.set_title(\"Classif loss\", fontsize=24)\n",
    "        ax.set_ylabel(\"Cross Entropy\")\n",
    "        \n",
    "        ax = axs[2]\n",
    "        sns.lineplot(\n",
    "            x=\"step\",\n",
    "            y=\"val\",\n",
    "            hue=\"group\",\n",
    "            data=loss_df[loss_df.variable == \"classif_acc\"],\n",
    "            ci=None,\n",
    "            legend=None,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.set_title(\"Classif acc\", fontsize=24)\n",
    "        ax.set_ylabel(\"Acc\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.884Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No objects to concatenate\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=79), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=79), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=79), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_umap_classif_results(model, X_valid, Y_valid, X_train, X_labeled, Y_labeled, batch_size, dralg=PCA, cmap='tab10', cmap2='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.886Z"
    }
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "n_neighbors=15\n",
    "batch_size = 256 # 500\n",
    "max_sample_repeats_per_epoch = 25\n",
    "max_save_delta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.888Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import build_fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.889Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.paths import MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.891Z"
    }
   },
   "outputs": [],
   "source": [
    "# where to save checkpoints\n",
    "cpt_path = MODEL_DIR / 'semisupervised' / dataset / datestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.892Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCHES_PER_EPOCH = int(n_edges_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.894Z"
    }
   },
   "outputs": [],
   "source": [
    "patience = 10000 # wait this many batches without improvement before early stopping\n",
    "min_delta = 0.0001 # threshold for what counts as an improvement\n",
    "best_acc = 0 # the best current accuracy score\n",
    "best_saved_acc = 0 # best accuracy on valid data that has been checkpointed\n",
    "best_saved_batch = 0 # batch number for last good batch\n",
    "max_reinitialize_delta = .01 # minimum loss in accuracy resulting in reinitialized weights\n",
    "plot_frequency = 1000 # how frequently to plot\n",
    "max_reloads = 10 # how often can we reload weights?\n",
    "batch = 0; epoch = 0\n",
    "batches_since_last_reload = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.896Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs_without_improvement = 0\n",
    "max_epochs_without_improvement = 5 # allow the network \n",
    "reload_tolerance = 100 # allow the network at least reload_tolerance batches before reloading last weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.897Z"
    }
   },
   "outputs": [],
   "source": [
    "n_reloads = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.899Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "improved_this_epoch = True\n",
    "for edge_epoch, epoch in tqdm(\n",
    "    zip(edge_iter, np.arange(N_EPOCHS)), total=N_EPOCHS, desc=\"epoch\"\n",
    "):\n",
    "\n",
    "    pbar = tqdm(total=BATCHES_PER_EPOCH, leave=False, desc=\"loss\")\n",
    "\n",
    "    for (batch_to, batch_from), (X, y), (X_val, y_val) in zip(\n",
    "        edge_epoch, labeled_iter, data_valid\n",
    "    ):\n",
    "\n",
    "        # train\n",
    "        (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            umap_loss,\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        ) = model.train(\n",
    "            batch_to=X_train[np.array(batch_to)],\n",
    "            batch_from=X_train[np.array(batch_from)],\n",
    "            X=X,\n",
    "            y=y,\n",
    "            umap_prop=umap_prop,\n",
    "        )\n",
    "\n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = compute_classifier_loss(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            model.encoder,\n",
    "            model.classifier,\n",
    "            model.sparse_ce,\n",
    "            model.class_acc_val,\n",
    "        )\n",
    "\n",
    "        pbar.update()\n",
    "        pbar.set_description(\"val acc: {}\".format(str(round(val_acc.numpy(), 5))))\n",
    "\n",
    "        # save losses\n",
    "        model.write_losses(\n",
    "            tf.convert_to_tensor(batch, dtype=tf.int64),\n",
    "            classifier_acc,\n",
    "            classifier_loss,\n",
    "            umap_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "        )\n",
    "\n",
    "        # plot results\n",
    "        if batch % plot_frequency == 0:\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "            print(\n",
    "                \"batch: {} | train acc: {} | val acc: {}\".format(\n",
    "                    str(batch),\n",
    "                    str(round(classifier_acc.numpy(), 4)),\n",
    "                    str(round(val_acc.numpy(), 4)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # make any improvement isn't a fluke of small batch size\n",
    "        if batches_since_last_reload >= reload_tolerance:\n",
    "\n",
    "            # save if validation accuracy went up\n",
    "            if val_acc > best_saved_acc + max_save_delta:\n",
    "                # save weights\n",
    "                print(\n",
    "                    \"saving weights | batch = {} | acc = {}\".format(\n",
    "                        batch, round(val_acc.numpy() * 100, 3)\n",
    "                    )\n",
    "                )\n",
    "                model.encoder.save_weights((cpt_path / \"encoder\").as_posix())\n",
    "                model.classifier.save_weights((cpt_path / \"classifier\").as_posix())\n",
    "                best_saved_acc = val_acc.numpy()\n",
    "                # save batch number\n",
    "                best_saved_batch = copy.deepcopy(batch)\n",
    "\n",
    "            # reinitialize if validation accuracy went down\n",
    "            if val_acc < (best_saved_acc - max_reinitialize_delta):\n",
    "                # if\n",
    "                if batches_since_last_reload >= reload_tolerance:\n",
    "                    batches_since_last_reload = 0\n",
    "                    lr = model.optimizer.lr.numpy()\n",
    "                    print(\n",
    "                        \"batch {} | acc = {} | learn rate = {} | reloading weights from batch {} at acc {} | n reloads = {}\".format(\n",
    "                            batch,\n",
    "                            str(round(val_acc.numpy(), 3)),\n",
    "                            lr,\n",
    "                            best_saved_batch,\n",
    "                            int(best_saved_acc * 100),\n",
    "                            n_reloads,\n",
    "                        )\n",
    "                    )\n",
    "                    # reload weights\n",
    "                    model.encoder.load_weights((cpt_path / \"encoder\").as_posix())\n",
    "                    model.classifier.load_weights((cpt_path / \"classifier\").as_posix())\n",
    "\n",
    "                    # set new learning rate\n",
    "                    model.optimizer.lr.assign(lr / 2)\n",
    "\n",
    "                    batch += 1\n",
    "                    n_reloads += 1\n",
    "\n",
    "                    # stop if reloaded too many times\n",
    "                    if n_reloads >= max_reloads:\n",
    "                        break\n",
    "\n",
    "            # if there is an imporovement, set new best score\n",
    "            if val_acc > best_acc + min_delta:\n",
    "                improved_this_epoch = True\n",
    "                last_improvement = 0\n",
    "                best_acc = val_acc\n",
    "            # if the model has not improved in patience batches, end epoch\n",
    "            elif last_improvement >= patience:\n",
    "                last_improvement = 0\n",
    "                print(\n",
    "                    \"leaving epoch because no improvement in {} batches\".format(\n",
    "                        patience\n",
    "                    )\n",
    "                )\n",
    "                break\n",
    "            else:\n",
    "                last_improvement += 1\n",
    "\n",
    "        # up the ticker\n",
    "        batch += 1\n",
    "        batches_since_last_reload += 1\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.901Z"
    }
   },
   "outputs": [],
   "source": [
    "test_enc = model_data(X_test, batch_size, model.encoder)\n",
    "y_pred = model_data(test_enc, batch_size, model.classifier)\n",
    "test_acc = tf.keras.metrics.sparse_categorical_accuracy(\n",
    "    Y_test, y_pred\n",
    ").numpy()\n",
    "print(np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.905Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(cpt_path / 'test_acc.npy', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.906Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_model = tf.keras.models.Model(model.classifier.input,[model.classifier.get_layer(name=\"dense_1\").get_output_at(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.908Z"
    }
   },
   "outputs": [],
   "source": [
    "z_enc = model_data(X_train, 100, encoder)\n",
    "last_layer_class = model_data(z_enc, batch_size, pred_model)\n",
    "z_pred = model_data(z_enc, batch_size, classifier)\n",
    "confidence = np.max(z_pred, axis = 1)\n",
    "umap_class = UMAP(verbose=True).fit_transform(last_layer_class, Y_masked)\n",
    "classifications = np.argmax(z_pred, axis=1)\n",
    "correctness = classifications == Y_train\n",
    "np.save(cpt_path / 'train_last_layer_class.npy', last_layer_class)\n",
    "np.save(cpt_path / 'train_z_enc.npy', z_enc)\n",
    "np.save(cpt_path / 'train_last_layer_class_umap.npy', umap_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.910Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(umap_class[:,0], umap_class[:,1], c =Y_train,  cmap = plt.cm.tab10, s = 1, alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-08T16:43:15.912Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(umap_class[:,0], umap_class[:,1], c =correctness,  cmap = plt.cm.viridis, s = 1, alpha = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

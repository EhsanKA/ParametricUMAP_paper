{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load data\n",
    "- build labeled iterator\n",
    "- build validation iterator\n",
    "- build networks\n",
    "- load pretrained network\n",
    "- build PUMAP\n",
    "- for each epoch\n",
    "  - build umap graph\n",
    "  - create data iterator\n",
    "  - train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T22:20:45.161877Z",
     "start_time": "2020-08-05T22:20:45.149500Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T22:20:45.171354Z",
     "start_time": "2020-08-05T22:20:45.163219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T22:20:48.134979Z",
     "start_time": "2020-08-05T22:20:45.172167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T22:20:48.164323Z",
     "start_time": "2020-08-05T22:20:48.136328Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T22:20:48.194014Z",
     "start_time": "2020-08-05T22:20:48.166026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar10_1.0_1024_64____2020_08_05_15_20_48_192178__learned-metric\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dataset = \"cifar10\"\n",
    "dims = (32, 32, 3)\n",
    "umap_prop = 1.0\n",
    "num_classes = 10\n",
    "PROJECTION_DIMS = 1024\n",
    "labels_per_class = 64  #'full'\n",
    "datestring = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "datestring = (\n",
    "    str(dataset)\n",
    "    + \"_\"\n",
    "    + str(umap_prop)\n",
    "    + \"_\"\n",
    "    + str(PROJECTION_DIMS)\n",
    "    + \"_\"\n",
    "    + str(labels_per_class)\n",
    "    + \"____\"\n",
    "    + datestring\n",
    "    + \"__learned-metric\"\n",
    ")\n",
    "print(datestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.127Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import umap\n",
    "import copy\n",
    "import os, tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.129Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.load_datasets import load_CIFAR10, mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.130Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_CIFAR10(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.132Z"
    }
   },
   "outputs": [],
   "source": [
    "if labels_per_class == \"full\":\n",
    "    X_labeled = X_train\n",
    "    Y_masked = Y_labeled = Y_train\n",
    "else:\n",
    "    X_labeled, Y_labeled, Y_masked = mask_labels(\n",
    "        X_train, Y_train, labels_per_class=labels_per_class\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.133Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import compute_cross_entropy, get_graph_elements\n",
    "from tfumap.semisupervised import create_edge_iterator, create_validation_iterator, create_classification_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.135Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.136Z"
    }
   },
   "outputs": [],
   "source": [
    "# make sure batch size is no bigger than the number of labels per class\n",
    "labeled_batch_size = batch_size if batch_size < len(Y_labeled) else len(Y_labeled)\n",
    "labeled_batch_size = 128\n",
    "labeled_iter = create_classification_iterator(X_labeled, Y_labeled, batch_size=batch_size)\n",
    "print(labeled_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.137Z"
    }
   },
   "outputs": [],
   "source": [
    "data_valid, n_valid_samp = create_validation_iterator(X_valid, Y_valid, batch_size, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.139Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import MaxPool2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "block_layers_num = 3\n",
    "weight_decay = 1e-4\n",
    "name = 'resnet20'\n",
    "def conv2d_bn(x, filters, kernel_size, weight_decay=.0, strides=(1, 1)):\n",
    "    layer = Conv2D(filters=filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   strides=strides,\n",
    "                   padding='same',\n",
    "                   use_bias=False,\n",
    "                   kernel_regularizer=l2(weight_decay)\n",
    "                   )(x)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv2d_bn_relu(x, filters, kernel_size, weight_decay=.0, strides=(1, 1)):\n",
    "    layer = conv2d_bn(x, filters, kernel_size, weight_decay, strides)\n",
    "    layer = Activation('relu')(layer)\n",
    "    return layer\n",
    "\n",
    "\n",
    "def ResidualBlock(x, filters, kernel_size, weight_decay, downsample=True):\n",
    "    if downsample:\n",
    "        # residual_x = conv2d_bn_relu(x, filters, kernel_size=1, strides=2)\n",
    "        residual_x = conv2d_bn(x, filters, kernel_size=1, strides=2)\n",
    "        stride = 2\n",
    "    else:\n",
    "        residual_x = x\n",
    "        stride = 1\n",
    "    residual = conv2d_bn_relu(x,\n",
    "                              filters=filters,\n",
    "                              kernel_size=kernel_size,\n",
    "                              weight_decay=weight_decay,\n",
    "                              strides=stride,\n",
    "                              )\n",
    "    residual = conv2d_bn(residual,\n",
    "                         filters=filters,\n",
    "                         kernel_size=kernel_size,\n",
    "                         weight_decay=weight_decay,\n",
    "                         strides=1,\n",
    "                         )\n",
    "    out = layers.add([residual_x, residual])\n",
    "    out = Activation('relu')(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "input_ = Input(shape=dims)\n",
    "x =input_\n",
    "x = conv2d_bn_relu(x, filters=16, kernel_size=(3, 3), weight_decay=weight_decay, strides=(1, 1))\n",
    "\n",
    "# # conv 2\n",
    "for i in range(block_layers_num):\n",
    "    x = ResidualBlock(x, filters=16, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "# # conv 3\n",
    "x = ResidualBlock(x, filters=32, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "for i in range(block_layers_num - 1):\n",
    "    x = ResidualBlock(x, filters=32, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "# # conv 4\n",
    "x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=True)\n",
    "for i in range(block_layers_num - 1):\n",
    "    x = ResidualBlock(x, filters=64, kernel_size=(3, 3), weight_decay=weight_decay, downsample=False)\n",
    "x = AveragePooling2D(pool_size=(8, 8), padding='valid')(x)\n",
    "x = Flatten()(x)\n",
    "encoder = Model(input_, x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.141Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential()\n",
    "classifier.add(tf.keras.layers.InputLayer(input_shape=64))\n",
    "classifier.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "classifier.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "classifier.add(tf.keras.layers.Dense(num_classes, activation='softmax', name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.142Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = tf.keras.Sequential()\n",
    "embedder.add(tf.keras.Input(shape=(64)))\n",
    "embedder.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "embedder.add(tf.keras.layers.Dense(units=256, activation=\"relu\"))\n",
    "embedder.add(tf.keras.layers.Dense(PROJECTION_DIMS, activation=None, name='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.144Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.145Z"
    }
   },
   "outputs": [],
   "source": [
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.147Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.paths import MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.148Z"
    }
   },
   "outputs": [],
   "source": [
    "load_loc = \"cifar10_0.0_1024_64____2020_07_31_17_11_05_811717\"\n",
    "encoder.load_weights((MODEL_DIR / 'semisupervised' / dataset / load_loc / \"encoder\").as_posix())\n",
    "classifier.load_weights((MODEL_DIR / 'semisupervised' / dataset / load_loc / \"classifier\").as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at pretrained state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.150Z"
    }
   },
   "outputs": [],
   "source": [
    "def model_data(X, batch_size, model):\n",
    "    \"\"\" embed a set of points in X to Z\n",
    "    \"\"\"\n",
    "    n_batch = int(np.ceil(len(X) / batch_size))\n",
    "    return np.vstack(\n",
    "        [\n",
    "                model(np.array(X[(i) * batch_size : (i + 1) * batch_size, :]))\n",
    "            for i in tqdm(range(n_batch), leave=False)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.151Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_model = tf.keras.models.Model(classifier.input,[classifier.get_layer(name=\"dense_1\").get_output_at(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.153Z"
    }
   },
   "outputs": [],
   "source": [
    "z_enc = model_data(X_valid, 100, encoder)\n",
    "last_layer_class = model_data(z_enc, batch_size, pred_model)\n",
    "z_pred = model_data(z_enc, batch_size, classifier)\n",
    "confidence = np.max(z_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.155Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_acc = tf.keras.metrics.sparse_categorical_accuracy(\n",
    "    Y_valid, z_pred\n",
    ").numpy()\n",
    "print(np.mean(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.157Z"
    }
   },
   "outputs": [],
   "source": [
    "umap_class = UMAP(verbose=True).fit_transform(last_layer_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.158Z"
    }
   },
   "outputs": [],
   "source": [
    "conf_thresh =  0.99\n",
    "conf_mask = confidence > conf_thresh\n",
    "umap_class_conf = UMAP(verbose=True).fit_transform(last_layer_class[conf_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.159Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols = 4, figsize=(24,5))\n",
    "axs[0].scatter(umap_class[:,0], umap_class[:,1], s=1, c = Y_valid, cmap = plt.cm.tab10)\n",
    "axs[0].set_title('UMAP last layer')\n",
    "axs[1].scatter(umap_class[:,0], umap_class[:,1], s=1, c = confidence, cmap = plt.cm.viridis)\n",
    "axs[1].set_title('Confidence')\n",
    "axs[2].scatter(umap_class_conf[:,0], umap_class_conf[:,1], s=1, c = Y_valid[conf_mask], cmap = plt.cm.tab10)\n",
    "axs[2].set_title('UMAP (conf >{})'.format(conf_thresh) )\n",
    "axs[3].hist((confidence), bins = 50);\n",
    "axs[3].set_title('Confidence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create UMAP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.161Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_model import PUMAP, compute_classifier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.162Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfumap.umap import compute_cross_entropy, convert_distance_to_probability\n",
    "from tfumap.semisupervised import (\n",
    "    find_a_b,\n",
    "    compute_umap_loss,\n",
    "    # compute_classifier_loss,\n",
    "    batch_data,\n",
    ")\n",
    "import numpy as np\n",
    "import os, tempfile\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_classifier_loss(X, y, encoder, classifier, sparse_ce, acc_func):\n",
    "    \"\"\" compute the cross entropy loss for classification\n",
    "        \"\"\"\n",
    "    d = classifier(encoder(X))\n",
    "    loss = sparse_ce(y, d)\n",
    "    acc = acc_func(y, d)\n",
    "    #acc = tf.keras.metrics.sparse_categorical_accuracy(y, d)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "class PUMAP(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        embedder,\n",
    "        classifier,\n",
    "        tensorboard_logdir=None,  # directory for tensorboard log\n",
    "        min_dist=0.1,\n",
    "        negative_sample_rate=5.0,\n",
    "        optimizer=tf.keras.optimizers.SGD(0.1),\n",
    "        repulsion_strength=1.0,\n",
    "        # ** kwargs,\n",
    "    ):\n",
    "        super(PUMAP, self).__init__()\n",
    "        # self.__dict__.update(kwargs)\n",
    "\n",
    "        # subnetworks\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "\n",
    "        # optimizer for cross entropy minimization\n",
    "        self.optimizer = optimizer\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "\n",
    "        # get a,b for current min_dist\n",
    "        self._a, self._b = find_a_b(min_dist)\n",
    "\n",
    "        # create summary writer to log loss information during training\n",
    "        if tensorboard_logdir is None:\n",
    "            self.tensorboard_logdir = os.path.join(\n",
    "                tempfile.gettempdir(),\n",
    "                \"tensorboard\",\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            )\n",
    "        else:\n",
    "            self.tensorboard_logdir = tensorboard_logdir\n",
    "        self.summary_writer_train = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/train\"\n",
    "        )\n",
    "        self.summary_writer_valid = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/valid\"\n",
    "        )\n",
    "\n",
    "        # sparse categorical cross entropy\n",
    "        self.sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.class_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.class_acc_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        # self.create_summary_metrics()\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, batch_to, batch_from, X, y, umap_prop = 1.0, save_loss=False):\n",
    "        \"\"\" One training step \n",
    "        Input are points and weights for positive and negative \n",
    "        samples for training. \n",
    "            \n",
    "        \"\"\"\n",
    "        if self.umap_prop > 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                (attraction_loss, repellant_loss, umap_loss) = compute_umap_loss(\n",
    "                    batch_to,\n",
    "                    batch_from,\n",
    "                    self.embedder,\n",
    "                    self.encoder,\n",
    "                    self._a,\n",
    "                    self._b,\n",
    "                    self.negative_sample_rate,\n",
    "                    self.repulsion_strength,\n",
    "                )\n",
    "            \n",
    "                classifier_loss, classifier_acc = compute_classifier_loss(\n",
    "                    X, y, self.encoder, self.classifier, self.sparse_ce, self.class_acc\n",
    "                )\n",
    "                loss = (\n",
    "                    tf.reduce_mean(classifier_loss)\n",
    "                    + tf.reduce_mean(umap_loss) * umap_prop\n",
    "                )\n",
    "\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables\n",
    "                + self.embedder.trainable_variables\n",
    "                + self.classifier.trainable_variables\n",
    "            )\n",
    "        else:  # ignore running costly UMAP computations if not training on UMAP loss\n",
    "            with tf.GradientTape() as tape:\n",
    "                classifier_loss, classifier_acc = compute_classifier_loss(\n",
    "                    X, y, self.encoder, self.classifier, self.sparse_ce, self.class_acc\n",
    "                )\n",
    "                loss = classifier_loss\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.classifier.trainable_variables\n",
    "            )\n",
    "            attraction_loss = repellant_loss = umap_loss = 0\n",
    "\n",
    "        # compute gradient for umap\n",
    "        grad = tape.gradient(loss, train_vars)\n",
    "\n",
    "        # gradients are cliped in UMAP implementation. Any effect here?\n",
    "        grad = [tf.clip_by_value(grad, -4.0, 4.0) for grad in grad]\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(grad, train_vars))\n",
    "\n",
    "        return (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            tf.reduce_mean(umap_loss),\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def write_losses(\n",
    "        self,\n",
    "        step,\n",
    "        classifier_acc,\n",
    "        classifier_loss,\n",
    "        umap_loss,\n",
    "        classifier_loss_val,\n",
    "        classifier_acc_val,\n",
    "    ):\n",
    "        # write train loss\n",
    "        with self.summary_writer_train.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"classif_acc\", classifier_acc, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\", classifier_loss, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"umap_loss\", umap_loss, step=step,\n",
    "            )\n",
    "            self.summary_writer_train.flush()\n",
    "        # write valid loss\n",
    "        with self.summary_writer_valid.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"classif_acc\", classifier_acc_val, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\", classifier_loss_val, step=step,\n",
    "            )\n",
    "            self.summary_writer_valid.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.164Z"
    }
   },
   "outputs": [],
   "source": [
    "### Initialize model\n",
    "model = PUMAP(\n",
    "    min_dist = 0.0,\n",
    "    negative_sample_rate = 5, # how many negative samples per positive\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3), # cross-entropy optimizer\n",
    "    encoder=encoder,\n",
    "    embedder=embedder,\n",
    "    classifier=classifier,\n",
    "    umap_prop=umap_prop\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.166Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import retrieve_tensors\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "from umap import UMAP \n",
    "\n",
    "def plot_umap_classif_results(\n",
    "    model,\n",
    "    X_valid,\n",
    "    Y_valid,\n",
    "    X_train,\n",
    "    X_labeled,\n",
    "    Y_labeled,\n",
    "    batch_size,\n",
    "    dralg = PCA, #umap\n",
    "    cmap=\"coolwarm\",\n",
    "    cmap2=\"bwr\",\n",
    "):\n",
    "    # get loss dataframe from tensorboard\n",
    "    try:\n",
    "        loss_df = retrieve_tensors(model.tensorboard_logdir)\n",
    "        losses_exist = True\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        losses_exist = False\n",
    "\n",
    "    # embed data\n",
    "    enc_valid = model_data(X_valid, batch_size, model.encoder)\n",
    "    embed_valid = model_data(enc_valid, batch_size, model.embedder)\n",
    "    class_valid = model_data(enc_valid, batch_size, model.classifier)\n",
    "\n",
    "    enc_lab = model_data(X_labeled, batch_size, model.encoder)\n",
    "    embed_lab = model_data(enc_lab, batch_size, model.embedder)\n",
    "    class_lab = model_data(enc_lab, batch_size, model.classifier)\n",
    "\n",
    "    # latent_valid = model.encoder(X_valid)\n",
    "    #dr = PCA()\n",
    "    dr = dralg()\n",
    "    enc_valid_2 = dr.fit_transform(enc_valid)\n",
    "    enc_lab_2 = dr.transform(enc_lab)\n",
    "    \n",
    "    # latent_valid = model.encoder(X_valid)\n",
    "    #dr = PCA()\n",
    "    dr = dralg()\n",
    "    embed_valid_2 = dr.fit_transform(embed_valid)\n",
    "    embed_lab_2 = dr.transform(embed_lab)\n",
    "    \n",
    "    # latent_valid = model.encoder(X_valid)\n",
    "    #dr = PCA()\n",
    "    dr = dralg()\n",
    "    class_valid_2 = dr.fit_transform(class_valid)\n",
    "    class_lab_2 = dr.transform(class_lab)\n",
    "\n",
    "    # plot\n",
    "    fig, axs = plt.subplots(ncols=6, figsize=(30, 4))\n",
    "    ax = axs[0]\n",
    "\n",
    "    ax.scatter(\n",
    "        enc_valid_2[:, 0],\n",
    "        enc_valid_2[:, 1],\n",
    "        c=Y_valid,\n",
    "        cmap=cmap,  # \"tab10\"\n",
    "        s=2,\n",
    "        alpha=0.25,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    \n",
    "    ax.scatter(\n",
    "        enc_lab_2[:, 0],\n",
    "        enc_lab_2[:, 1],\n",
    "        c=Y_labeled,\n",
    "        cmap=cmap,  # \"tab10\"\n",
    "        s=20,\n",
    "        alpha=0.25,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"PCA of Encoder\", fontsize=24)\n",
    "    ax.axis(\"equal\")\n",
    "    \n",
    "    ax = axs[1]\n",
    "\n",
    "    ax.scatter(\n",
    "        embed_valid_2[:, 0],\n",
    "        embed_valid_2[:, 1],\n",
    "        c=Y_valid,\n",
    "        cmap=cmap,  # \"tab10\"\n",
    "        s=2,\n",
    "        alpha=0.25,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    \n",
    "    ax.scatter(\n",
    "        embed_lab_2[:, 0],\n",
    "        embed_lab_2[:, 1],\n",
    "        c=Y_labeled,\n",
    "        cmap=cmap,  # \"tab10\"\n",
    "        s=20,\n",
    "        alpha=0.25,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"PCA of Embed\", fontsize=24)\n",
    "    ax.axis(\"equal\")\n",
    "    \n",
    "    ax = axs[2]\n",
    "\n",
    "    ax.scatter(\n",
    "        class_valid_2[:, 0],\n",
    "        class_valid_2[:, 1],\n",
    "        c=Y_valid,\n",
    "        cmap=cmap,  # \"tab10\"\n",
    "        s=2,\n",
    "        alpha=0.25,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    \n",
    "    ax.scatter(\n",
    "        class_lab_2[:, 0],\n",
    "        class_lab_2[:, 1],\n",
    "        c=Y_labeled,\n",
    "        cmap=cmap,  # \"tab10\"\n",
    "        s=20,\n",
    "        alpha=0.25,\n",
    "        rasterized=True,\n",
    "    )\n",
    "    \n",
    "    ax.set_title(\"PCA of class\", fontsize=24)\n",
    "    ax.axis(\"equal\")\n",
    "    \n",
    "    \n",
    "    if losses_exist:\n",
    "        ax = axs[3]\n",
    "        sns.lineplot(\n",
    "            x=\"step\",\n",
    "            y=\"val\",\n",
    "            hue=\"group\",\n",
    "            data=loss_df[loss_df.variable == \"umap_loss\"],\n",
    "            ci=None,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.set_title(\"UMAP loss\", fontsize=24)\n",
    "        ax.set_ylabel(\"Cross Entropy\")\n",
    "        ax = axs[4]\n",
    "        sns.lineplot(\n",
    "            x=\"step\",\n",
    "            y=\"val\",\n",
    "            hue=\"group\",\n",
    "            data=loss_df[loss_df.variable == \"classif_loss\"],\n",
    "            ci=None,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.legend(loc=\"upper right\")\n",
    "        # ax.set_yscale(\"log\")\n",
    "        ax.set_title(\"Classif loss\", fontsize=24)\n",
    "        ax.set_ylabel(\"Cross Entropy\")\n",
    "        ax = axs[5]\n",
    "        sns.lineplot(\n",
    "            x=\"step\",\n",
    "            y=\"val\",\n",
    "            hue=\"group\",\n",
    "            data=loss_df[loss_df.variable == \"classif_acc\"],\n",
    "            ci=None,\n",
    "            ax=ax,\n",
    "        )\n",
    "        ax.set_xscale(\"log\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "        ax.set_title(\"Classif acc\", fontsize=24)\n",
    "        ax.set_ylabel(\"Acc\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.168Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_umap_classif_results(model, X_valid, Y_valid, X_train, X_labeled, Y_labeled, batch_size, cmap='tab10', cmap2='tab10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.170Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_elements(batch_to, batch_from):\n",
    "    return tf.gather(X_train, batch_to), tf.gather(X_train, batch_from)\n",
    "\n",
    "def create_edge_iterator(\n",
    "    head, tail, weight, batch_size, max_sample_repeats_per_epoch=25\n",
    "):\n",
    "    \"\"\" create an iterator for edges\n",
    "    \"\"\"\n",
    "    # set the maximum number of times each edge should be repeated per epoch\n",
    "    epochs_per_sample = np.clip(\n",
    "        (weight / np.max(weight)) * max_sample_repeats_per_epoch,\n",
    "        1,\n",
    "        max_sample_repeats_per_epoch,\n",
    "    ).astype(\"int\")\n",
    "\n",
    "    edges_to_exp, edges_from_exp = (\n",
    "        np.repeat(head, epochs_per_sample.astype(\"int\")),\n",
    "        np.repeat(tail, epochs_per_sample.astype(\"int\")),\n",
    "    )\n",
    "\n",
    "    # permutate\n",
    "    perm_mask = np.random.permutation(range(len(edges_to_exp)))\n",
    "    edges_to_exp = edges_to_exp[perm_mask]\n",
    "    edges_from_exp = edges_from_exp[perm_mask]\n",
    "\n",
    "    # make dataset\n",
    "    edge_iter = (\n",
    "        tf.data.Dataset.from_tensor_slices((edges_to_exp, edges_from_exp))\n",
    "        #.shuffle(len(edges_to_exp))\n",
    "        .batch(batch_size)\n",
    "        .map(get_elements)\n",
    "    )\n",
    "\n",
    "    return edge_iter, len(edges_to_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.171Z"
    }
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "n_neighbors=15\n",
    "batch_size = 500\n",
    "max_sample_repeats_per_epoch = 25\n",
    "max_save_delta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.172Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import build_fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.173Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.paths import MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.175Z"
    }
   },
   "outputs": [],
   "source": [
    "# where to save checkpoints\n",
    "cpt_path = MODEL_DIR / 'semisupervised' / dataset / datestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.176Z"
    }
   },
   "outputs": [],
   "source": [
    "patience = 10000 # wait this many batches without improvement before early stopping\n",
    "min_delta = 0.0001 # threshold for what counts as an improvement\n",
    "best_acc = 0 # the best current accuracy score\n",
    "best_saved_acc = 0 # best accuracy on valid data that has been checkpointed\n",
    "best_saved_batch = 0 # batch number for last good batch\n",
    "max_reinitialize_delta = .01 # minimum loss in accuracy resulting in reinitialized weights\n",
    "plot_frequency = 1000 # how frequently to plot\n",
    "max_reloads = 10 # how often can we reload weights?\n",
    "batch = 0; epoch = 0\n",
    "batches_since_last_reload = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.177Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs_without_improvement = 0\n",
    "max_epochs_without_improvement = 5 # allow the network \n",
    "reload_tolerance = 100 # allow the network at least reload_tolerance batches before reloading last weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.179Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_model = tf.keras.models.Model(\n",
    "        model.classifier.input,\n",
    "        [model.classifier.get_layer(name=\"dense_1\").get_output_at(0)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.182Z"
    }
   },
   "outputs": [],
   "source": [
    "improved_this_epoch = True\n",
    "for epoch in tqdm(zip(np.arange(N_EPOCHS)), total=N_EPOCHS, desc=\"epoch\"):\n",
    "\n",
    "    if not improved_this_epoch:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement > max_epochs_without_improvement:\n",
    "        print(\n",
    "            \"Early stopping because no improvement in {} epochs\".format(\n",
    "                max_epochs_without_improvement\n",
    "            )\n",
    "        )\n",
    "        break\n",
    "\n",
    "    # reset improvement information\n",
    "    improved_this_epoch = False\n",
    "    last_improvement = (\n",
    "        0  # delta between current batch, and the last batch that was an improvement\n",
    "    )\n",
    "    n_reloads = 0\n",
    "    batches_since_last_reload = 0\n",
    "\n",
    "    ####  build graph\n",
    "    # embed data\n",
    "    train_enc = model_data(X_train, 100, model.encoder)\n",
    "    train_last_layer = model_data(train_enc, 100, pred_model)\n",
    "\n",
    "    # construct graph\n",
    "    umap_graph = build_fuzzy_simplicial_set(\n",
    "        train_last_layer, y=Y_masked, n_neighbors=n_neighbors,\n",
    "    )\n",
    "    # get graph elements\n",
    "    graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "        umap_graph, n_epochs=200\n",
    "    )\n",
    "\n",
    "    # create edge iterator\n",
    "    edge_epoch, n_edges_per_epoch = create_edge_iterator(\n",
    "        head,\n",
    "        tail,\n",
    "        weight,\n",
    "        batch_size=batch_size,\n",
    "        max_sample_repeats_per_epoch=max_sample_repeats_per_epoch,\n",
    "    )\n",
    "\n",
    "    BATCHES_PER_EPOCH = int(n_edges_per_epoch / batch_size)\n",
    "\n",
    "    pbar = tqdm(total=BATCHES_PER_EPOCH, leave=False, desc=\"loss\")\n",
    "\n",
    "    for (batch_to, batch_from), (X, y), (X_val, y_val) in zip(\n",
    "        edge_epoch, labeled_iter, data_valid\n",
    "    ):\n",
    "\n",
    "        # train\n",
    "        (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            umap_loss,\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        ) = model.train(batch_to=batch_to, batch_from=batch_from, X=X, y=y, umap_prop = umap_prop)\n",
    "\n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = compute_classifier_loss(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            model.encoder,\n",
    "            model.classifier,\n",
    "            model.sparse_ce,\n",
    "            model.class_acc_val,\n",
    "        )\n",
    "        \n",
    "        pbar.update()\n",
    "        pbar.set_description(\"val acc: {}\".format(str(round(val_acc.numpy(), 5))))\n",
    "\n",
    "        # save losses\n",
    "        model.write_losses(\n",
    "            tf.convert_to_tensor(batch, dtype=tf.int64),\n",
    "            classifier_acc,\n",
    "            classifier_loss,\n",
    "            umap_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "        )\n",
    "\n",
    "        # plot results\n",
    "        if batch % plot_frequency == 0:\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "            print(\n",
    "                \"batch: {} | train acc: {} | val acc: {}\".format(\n",
    "                    str(batch),\n",
    "                    str(round(classifier_acc.numpy(), 4)),\n",
    "                    str(round(val_acc.numpy(), 4)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # make any improvement isn't a fluke of small batch size\n",
    "        if batches_since_last_reload >= reload_tolerance:\n",
    "\n",
    "            # save if validation accuracy went up\n",
    "            if val_acc > best_saved_acc + max_save_delta:\n",
    "                # save weights\n",
    "                print(\n",
    "                    \"saving weights | batch = {} | acc = {}\".format(\n",
    "                        batch, round(val_acc.numpy() * 100, 3)\n",
    "                    )\n",
    "                )\n",
    "                model.encoder.save_weights((cpt_path / \"encoder\").as_posix())\n",
    "                model.embedder.save_weights((cpt_path / \"embedder\").as_posix())\n",
    "                model.classifier.save_weights((cpt_path / \"classifier\").as_posix())\n",
    "                best_saved_acc = val_acc.numpy()\n",
    "                # save batch number\n",
    "                best_saved_batch = copy.deepcopy(batch)\n",
    "\n",
    "            # reinitialize if validation accuracy went down\n",
    "            if val_acc < (best_saved_acc - max_reinitialize_delta):\n",
    "                # if\n",
    "                if batches_since_last_reload >= reload_tolerance:\n",
    "                    batches_since_last_reload = 0\n",
    "                    print(\n",
    "                        \"batch {} | acc = {} | learn rate = {} | reloading weights from batch {} at acc {} | n reloads = {}\".format(\n",
    "                            batch,\n",
    "                            str(round(val_acc.numpy(), 3)),\n",
    "                            lr,\n",
    "                            best_saved_batch,\n",
    "                            int(best_saved_acc * 100),\n",
    "                            n_reloads,\n",
    "                        )\n",
    "                    )\n",
    "                    # reload weights\n",
    "                    model.encoder.load_weights((cpt_path / \"encoder\").as_posix())\n",
    "                    model.embedder.load_weights((cpt_path / \"embedder\").as_posix())\n",
    "                    model.classifier.load_weights((cpt_path / \"classifier\").as_posix())\n",
    "\n",
    "                    # set new learning rate\n",
    "                    model.optimizer.lr.assign(lr / 2)\n",
    "\n",
    "                    batch += 1\n",
    "                    n_reloads += 1\n",
    "\n",
    "                    # stop if reloaded too many times\n",
    "                    if n_reloads >= max_reloads:\n",
    "                        break\n",
    "\n",
    "            # if there is an imporovement, set new best score\n",
    "            if val_acc > best_acc + min_delta:\n",
    "                improved_this_epoch = True\n",
    "                last_improvement = 0\n",
    "                best_acc = val_acc\n",
    "            # if the model has not improved in patience batches, end epoch\n",
    "            elif last_improvement >= patience:\n",
    "                last_improvement = 0\n",
    "                print(\n",
    "                    \"leaving epoch because no improvement in {} batches\".format(\n",
    "                        patience\n",
    "                    )\n",
    "                )\n",
    "                break\n",
    "            else:\n",
    "                last_improvement += 1\n",
    "\n",
    "        # up the ticker\n",
    "        batch += 1\n",
    "        batches_since_last_reload += 1\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.184Z"
    }
   },
   "outputs": [],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T22:05:14.266510Z",
     "start_time": "2020-08-05T22:05:13.621068Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.188Z"
    }
   },
   "outputs": [],
   "source": [
    "improved_this_epoch = True\n",
    "for epoch in tqdm(zip(np.arange(N_EPOCHS)), total=N_EPOCHS, desc=\"epoch\"):\n",
    "\n",
    "    if not improved_this_epoch:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement > max_epochs_without_improvement:\n",
    "        print(\n",
    "            \"Early stopping because no improvement in {} epochs\".format(\n",
    "                max_epochs_without_improvement\n",
    "            )\n",
    "        )\n",
    "        break\n",
    "\n",
    "    # reset improvement information\n",
    "    improved_this_epoch = False\n",
    "    last_improvement = (\n",
    "        0  # delta between current batch, and the last batch that was an improvement\n",
    "    )\n",
    "    n_reloads = 0\n",
    "    batches_since_last_reload = 0\n",
    "\n",
    "    ####  build graph\n",
    "    # embed data\n",
    "    train_enc = model_data(X_train, 100, model.encoder)\n",
    "    train_last_layer = model_data(train_enc, 100, pred_model)\n",
    "\n",
    "    # construct graph\n",
    "    umap_graph = build_fuzzy_simplicial_set(\n",
    "        train_last_layer, y=Y_masked, n_neighbors=n_neighbors,\n",
    "    )\n",
    "    # get graph elements\n",
    "    graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "        umap_graph, n_epochs=200\n",
    "    )\n",
    "\n",
    "    # create edge iterator\n",
    "    edge_epoch, n_edges_per_epoch = create_edge_iterator(\n",
    "        head,\n",
    "        tail,\n",
    "        weight,\n",
    "        batch_size=batch_size,\n",
    "        max_sample_repeats_per_epoch=max_sample_repeats_per_epoch,\n",
    "    )\n",
    "\n",
    "    BATCHES_PER_EPOCH = int(n_edges_per_epoch / batch_size)\n",
    "\n",
    "    pbar = tqdm(total=BATCHES_PER_EPOCH, leave=False, desc=\"loss\")\n",
    "\n",
    "    for (batch_to, batch_from), (X, y), (X_val, y_val) in zip(\n",
    "        edge_epoch, labeled_iter, data_valid\n",
    "    ):\n",
    "\n",
    "        # train\n",
    "        (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            umap_loss,\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        ) = model.train(batch_to=batch_to, batch_from=batch_from, X=X, y=y, umap_prop = 0.0)\n",
    "\n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = compute_classifier_loss(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            model.encoder,\n",
    "            model.classifier,\n",
    "            model.sparse_ce,\n",
    "            model.class_acc_val,\n",
    "        )\n",
    "        \n",
    "        pbar.update()\n",
    "        pbar.set_description(\"val acc: {}\".format(str(round(val_acc.numpy(), 5))))\n",
    "\n",
    "        # save losses\n",
    "        model.write_losses(\n",
    "            tf.convert_to_tensor(batch, dtype=tf.int64),\n",
    "            classifier_acc,\n",
    "            classifier_loss,\n",
    "            umap_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "        )\n",
    "\n",
    "        # plot results\n",
    "        if batch % plot_frequency == 0:\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "            print(\n",
    "                \"batch: {} | train acc: {} | val acc: {}\".format(\n",
    "                    str(batch),\n",
    "                    str(round(classifier_acc.numpy(), 4)),\n",
    "                    str(round(val_acc.numpy(), 4)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # make any improvement isn't a fluke of small batch size\n",
    "        if batches_since_last_reload >= reload_tolerance:\n",
    "\n",
    "            # save if validation accuracy went up\n",
    "            if val_acc > best_saved_acc + max_save_delta:\n",
    "                # save weights\n",
    "                print(\n",
    "                    \"saving weights | batch = {} | acc = {}\".format(\n",
    "                        batch, round(val_acc.numpy() * 100, 3)\n",
    "                    )\n",
    "                )\n",
    "                model.encoder.save_weights((cpt_path / \"encoder\").as_posix())\n",
    "                model.embedder.save_weights((cpt_path / \"embedder\").as_posix())\n",
    "                model.classifier.save_weights((cpt_path / \"classifier\").as_posix())\n",
    "                best_saved_acc = val_acc.numpy()\n",
    "                # save batch number\n",
    "                best_saved_batch = copy.deepcopy(batch)\n",
    "\n",
    "            # reinitialize if validation accuracy went down\n",
    "            if val_acc < (best_saved_acc - max_reinitialize_delta):\n",
    "                # if\n",
    "                if batches_since_last_reload >= reload_tolerance:\n",
    "                    batches_since_last_reload = 0\n",
    "                    print(\n",
    "                        \"batch {} | acc = {} | learn rate = {} | reloading weights from batch {} at acc {} | n reloads = {}\".format(\n",
    "                            batch,\n",
    "                            str(round(val_acc.numpy(), 3)),\n",
    "                            lr,\n",
    "                            best_saved_batch,\n",
    "                            int(best_saved_acc * 100),\n",
    "                            n_reloads,\n",
    "                        )\n",
    "                    )\n",
    "                    # reload weights\n",
    "                    model.encoder.load_weights((cpt_path / \"encoder\").as_posix())\n",
    "                    model.embedder.load_weights((cpt_path / \"embedder\").as_posix())\n",
    "                    model.classifier.load_weights((cpt_path / \"classifier\").as_posix())\n",
    "\n",
    "                    # set new learning rate\n",
    "                    model.optimizer.lr.assign(lr / 2)\n",
    "\n",
    "                    batch += 1\n",
    "                    n_reloads += 1\n",
    "\n",
    "                    # stop if reloaded too many times\n",
    "                    if n_reloads >= max_reloads:\n",
    "                        break\n",
    "\n",
    "            # if there is an imporovement, set new best score\n",
    "            if val_acc > best_acc + min_delta:\n",
    "                improved_this_epoch = True\n",
    "                last_improvement = 0\n",
    "                best_acc = val_acc\n",
    "            # if the model has not improved in patience batches, end epoch\n",
    "            elif last_improvement >= patience:\n",
    "                last_improvement = 0\n",
    "                print(\n",
    "                    \"leaving epoch because no improvement in {} batches\".format(\n",
    "                        patience\n",
    "                    )\n",
    "                )\n",
    "                break\n",
    "            else:\n",
    "                last_improvement += 1\n",
    "\n",
    "        # up the ticker\n",
    "        batch += 1\n",
    "        batches_since_last_reload += 1\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.189Z"
    }
   },
   "outputs": [],
   "source": [
    "z_enc = model_data(X_valid, 100, encoder)\n",
    "last_layer_class = model_data(z_enc, batch_size, pred_model)\n",
    "z_pred = model_data(z_enc, batch_size, classifier)\n",
    "confidence = np.max(z_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T22:20:45.190Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_acc = tf.keras.metrics.sparse_categorical_accuracy(\n",
    "    Y_valid, z_pred\n",
    ").numpy()\n",
    "print(np.mean(valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

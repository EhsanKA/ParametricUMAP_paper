{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:15.603563Z",
     "start_time": "2020-07-31T16:57:15.591364Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:15.628419Z",
     "start_time": "2020-07-31T16:57:15.604517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:20.913817Z",
     "start_time": "2020-07-31T16:57:15.630179Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:20.944148Z",
     "start_time": "2020-07-31T16:57:20.915111Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:20.974339Z",
     "start_time": "2020-07-31T16:57:20.945813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_1.0_1024_16____2020_07_31_09_57_20_972574\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dataset = 'mnist'\n",
    "dims = (28,28,1)\n",
    "umap_prop = 1.0\n",
    "num_classes = 10\n",
    "PROJECTION_DIMS = 1024\n",
    "labels_per_class = 16 # 'full'\n",
    "datestring = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "datestring = str(dataset) + '_' + str(umap_prop) + '_'  +  str(PROJECTION_DIMS)+ '_'  + str(labels_per_class) + '____' + datestring\n",
    "print(datestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:26.865569Z",
     "start_time": "2020-07-31T16:57:20.976043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import umap\n",
    "import copy\n",
    "import os, tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:26.999161Z",
     "start_time": "2020-07-31T16:57:26.866959Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.load_datasets import load_MNIST, mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:27.408929Z",
     "start_time": "2020-07-31T16:57:27.001152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_MNIST(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:27.451851Z",
     "start_time": "2020-07-31T16:57:27.409956Z"
    }
   },
   "outputs": [],
   "source": [
    "if labels_per_class == \"full\":\n",
    "    X_labeled = X_train\n",
    "    Y_masked = Y_labeled = Y_train\n",
    "else:\n",
    "    X_labeled, Y_labeled, Y_masked = mask_labels(\n",
    "        X_train, Y_train, labels_per_class=labels_per_class\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build umap graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-31T16:57:27.550294Z",
     "start_time": "2020-07-31T16:57:27.453055Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import build_fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.564Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neighbors = 15  # default = 15\n",
    "umap_graph = build_fuzzy_simplicial_set(\n",
    "    X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:]))),\n",
    "    y=Y_masked,\n",
    "    n_neighbors=n_neighbors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.566Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import compute_cross_entropy, get_graph_elements\n",
    "from tfumap.semisupervised import create_edge_iterator, create_validation_iterator, create_classification_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.568Z"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 200\n",
    "graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "            umap_graph, n_epochs\n",
    ")\n",
    "batch_size = np.min([n_vertices, 1000])\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.570Z"
    }
   },
   "outputs": [],
   "source": [
    "# make sure batch size is no bigger than the number of labels per class\n",
    "labeled_batch_size = batch_size if batch_size < len(Y_labeled) else len(Y_labeled)\n",
    "labeled_iter = create_classification_iterator(X_labeled, Y_labeled, batch_size=batch_size)\n",
    "print(labeled_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.571Z"
    }
   },
   "outputs": [],
   "source": [
    "max_sample_repeats_per_epoch = 25\n",
    "edge_iter, n_edges_per_epoch = create_edge_iterator(\n",
    "    head,\n",
    "    tail,\n",
    "    weight,\n",
    "    batch_size=batch_size,\n",
    "    max_sample_repeats_per_epoch=max_sample_repeats_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.573Z"
    }
   },
   "outputs": [],
   "source": [
    "data_valid, n_valid_samp = create_validation_iterator(X_valid, Y_valid, batch_size, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.575Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.Sequential()\n",
    "encoder.add(tf.keras.layers.InputLayer(input_shape=dims))\n",
    "encoder.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"\n",
    "))\n",
    "encoder.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\"\n",
    "))\n",
    "encoder.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "encoder.add(tf.keras.layers.Dropout(0.25))\n",
    "encoder.add(tf.keras.layers.Flatten())\n",
    "encoder.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "encoder.add(tf.keras.layers.Dropout(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.576Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential()\n",
    "classifier.add(tf.keras.layers.InputLayer(input_shape=128))\n",
    "classifier.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "classifier.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "classifier.add(tf.keras.layers.Dense(num_classes, activation='softmax', name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.578Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = tf.keras.Sequential()\n",
    "embedder.add(tf.keras.Input(shape=(128)))\n",
    "embedder.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "embedder.add(tf.keras.layers.Dense(units=128, activation=\"relu\"))\n",
    "embedder.add(tf.keras.layers.Dense(PROJECTION_DIMS, activation=None, name='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.579Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create UMAP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.581Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_model import PUMAP, compute_classifier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.583Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tfumap.umap import compute_cross_entropy, convert_distance_to_probability\n",
    "from tfumap.semisupervised import (\n",
    "    find_a_b,\n",
    "    compute_umap_loss,\n",
    "    # compute_classifier_loss,\n",
    "    batch_data,\n",
    ")\n",
    "import numpy as np\n",
    "import os, tempfile\n",
    "from tqdm.autonotebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def compute_classifier_loss(X, y, encoder, classifier, sparse_ce, acc_func):\n",
    "    \"\"\" compute the cross entropy loss for classification\n",
    "        \"\"\"\n",
    "    d = classifier(encoder(X))\n",
    "    loss = sparse_ce(y, d)\n",
    "    acc = acc_func(y, d)\n",
    "    #acc = tf.keras.metrics.sparse_categorical_accuracy(y, d)\n",
    "    return loss, acc\n",
    "\n",
    "\n",
    "class PUMAP(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder,\n",
    "        embedder,\n",
    "        classifier,\n",
    "        tensorboard_logdir=None,  # directory for tensorboard log\n",
    "        min_dist=0.1,\n",
    "        negative_sample_rate=5.0,\n",
    "        optimizer=tf.keras.optimizers.SGD(0.1),\n",
    "        repulsion_strength=1.0,\n",
    "        umap_prop=1.0,  # to what extent do we train UMAP\n",
    "        # ** kwargs,\n",
    "    ):\n",
    "        super(PUMAP, self).__init__()\n",
    "        # self.__dict__.update(kwargs)\n",
    "\n",
    "        # subnetworks\n",
    "        self.embedder = embedder\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "\n",
    "        self.umap_prop = umap_prop\n",
    "\n",
    "        # optimizer for cross entropy minimization\n",
    "        self.optimizer = optimizer\n",
    "        self.repulsion_strength = repulsion_strength\n",
    "        self.negative_sample_rate = negative_sample_rate\n",
    "\n",
    "        # get a,b for current min_dist\n",
    "        self._a, self._b = find_a_b(min_dist)\n",
    "\n",
    "        # create summary writer to log loss information during training\n",
    "        if tensorboard_logdir is None:\n",
    "            self.tensorboard_logdir = os.path.join(\n",
    "                tempfile.gettempdir(),\n",
    "                \"tensorboard\",\n",
    "                datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            )\n",
    "        else:\n",
    "            self.tensorboard_logdir = tensorboard_logdir\n",
    "        self.summary_writer_train = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/train\"\n",
    "        )\n",
    "        self.summary_writer_valid = tf.summary.create_file_writer(\n",
    "            self.tensorboard_logdir + \"/valid\"\n",
    "        )\n",
    "\n",
    "        # sparse categorical cross entropy\n",
    "        self.sparse_ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        self.class_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "        self.class_acc_val = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "        # self.create_summary_metrics()\n",
    "\n",
    "    @tf.function\n",
    "    def train(self, batch_to, batch_from, X, y, save_loss=False):\n",
    "        \"\"\" One training step \n",
    "        Input are points and weights for positive and negative \n",
    "        samples for training. \n",
    "            \n",
    "        \"\"\"\n",
    "        if self.umap_prop > 0:\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                (attraction_loss, repellant_loss, umap_loss) = compute_umap_loss(\n",
    "                    batch_to,\n",
    "                    batch_from,\n",
    "                    self.embedder,\n",
    "                    self.encoder,\n",
    "                    self._a,\n",
    "                    self._b,\n",
    "                    self.negative_sample_rate,\n",
    "                    self.repulsion_strength,\n",
    "                )\n",
    "            \n",
    "                classifier_loss, classifier_acc = compute_classifier_loss(\n",
    "                    X, y, self.encoder, self.classifier, self.sparse_ce, self.class_acc\n",
    "                )\n",
    "                loss = (\n",
    "                    tf.reduce_mean(classifier_loss)\n",
    "                    + tf.reduce_mean(umap_loss) * self.umap_prop\n",
    "                )\n",
    "\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables\n",
    "                + self.embedder.trainable_variables\n",
    "                + self.classifier.trainable_variables\n",
    "            )\n",
    "        else:  # ignore running costly UMAP computations if not training on UMAP loss\n",
    "            with tf.GradientTape() as tape:\n",
    "                classifier_loss, classifier_acc = compute_classifier_loss(\n",
    "                    X, y, self.encoder, self.classifier, self.sparse_ce, self.class_acc\n",
    "                )\n",
    "                loss = classifier_loss\n",
    "            train_vars = (\n",
    "                self.encoder.trainable_variables + self.classifier.trainable_variables\n",
    "            )\n",
    "            attraction_loss = repellant_loss = umap_loss = 0\n",
    "\n",
    "        # compute gradient for umap\n",
    "        grad = tape.gradient(loss, train_vars)\n",
    "\n",
    "        # gradients are cliped in UMAP implementation. Any effect here?\n",
    "        grad = [tf.clip_by_value(grad, -4.0, 4.0) for grad in grad]\n",
    "\n",
    "        self.optimizer.apply_gradients(zip(grad, train_vars))\n",
    "\n",
    "        return (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            tf.reduce_mean(umap_loss),\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        )\n",
    "\n",
    "    @tf.function\n",
    "    def write_losses(\n",
    "        self,\n",
    "        step,\n",
    "        classifier_acc,\n",
    "        classifier_loss,\n",
    "        umap_loss,\n",
    "        classifier_loss_val,\n",
    "        classifier_acc_val,\n",
    "    ):\n",
    "        # write train loss\n",
    "        with self.summary_writer_train.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"classif_acc\", classifier_acc, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\", classifier_loss, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"umap_loss\", umap_loss, step=step,\n",
    "            )\n",
    "            self.summary_writer_train.flush()\n",
    "        # write valid loss\n",
    "        with self.summary_writer_valid.as_default():\n",
    "            tf.summary.scalar(\n",
    "                \"classif_acc\", classifier_acc_val, step=step,\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"classif_loss\", classifier_loss_val, step=step,\n",
    "            )\n",
    "            self.summary_writer_valid.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.585Z"
    }
   },
   "outputs": [],
   "source": [
    "### Initialize model\n",
    "model = PUMAP(\n",
    "    min_dist = 0.0,\n",
    "    negative_sample_rate = 5, # how many negative samples per positive\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3), # cross-entropy optimizer\n",
    "    encoder=encoder,\n",
    "    embedder=embedder,\n",
    "    classifier=classifier,\n",
    "    umap_prop=umap_prop\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.587Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_plotting import plot_umap_classif_results, plot_results, get_decision_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.589Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_umap_classif_results(model, X_valid, Y_valid, X_train, X_labeled, Y_labeled, batch_size, cmap='tab10', cmap2='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.591Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.paths import MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.593Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = 0; epoch = 0\n",
    "N_EPOCHS = 10\n",
    "BATCHES_PER_EPOCH = int(n_edges_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.594Z"
    }
   },
   "outputs": [],
   "source": [
    "# early stopping parameters\n",
    "patience = 10000 # wait this many batches without improvement before early stopping\n",
    "min_delta = 0.0001 # threshold for what counts as an improvement\n",
    "best_acc = 0 # the best current accuracy score\n",
    "last_improvement = 0 # delta between current batch, and the last batch that was an improvement\n",
    "best_saved_acc = 0 # best accuracy on valid data that has been checkpointed\n",
    "best_saved_batch = 0 # batch number for last good batch\n",
    "max_reinitialize_delta = .01 # minimum loss in accuracy resulting in reinitialized weights\n",
    "plot_frequency = 2500 # how frequently to plot\n",
    "save_frequency = 2500 # how frequently to save batches\n",
    "early_stopping = False\n",
    "# where to save checkpoints\n",
    "cpt_path = MODEL_DIR / 'semisupervised' / dataset / datestring\n",
    "# how often can we reload weights?\n",
    "max_reloads = 10\n",
    "n_reloads = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.601Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for edge_epoch, epoch in tqdm(zip(edge_iter, np.arange(N_EPOCHS)), total=N_EPOCHS, desc = 'epoch'):\n",
    "    for (batch_to, batch_from), (X, y), (X_val, y_val) in tqdm(\n",
    "        zip(edge_epoch, labeled_iter, data_valid), total=BATCHES_PER_EPOCH, leave=False, desc = 'batch'\n",
    "    ):\n",
    "\n",
    "        # train\n",
    "        (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            umap_loss,\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        ) = model.train(\n",
    "            batch_to=X_train[batch_to], batch_from=X_train[batch_from], X=X, y=y\n",
    "        )\n",
    "\n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = compute_classifier_loss(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            model.encoder,\n",
    "            model.classifier,\n",
    "            model.sparse_ce,\n",
    "            model.class_acc_val,\n",
    "        )\n",
    "\n",
    "        # save losses\n",
    "        model.write_losses(\n",
    "            tf.convert_to_tensor(batch, dtype=tf.int64),\n",
    "            classifier_acc,\n",
    "            classifier_loss,\n",
    "            umap_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "        )\n",
    "\n",
    "        # plot results\n",
    "        if batch % plot_frequency == 0:\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "            print(\n",
    "                \"batch: {} | train acc: {} | val acc: {}\".format(\n",
    "                    str(batch),\n",
    "                    str(round(classifier_acc.numpy(), 4)),\n",
    "                    str(round(val_acc.numpy(), 4)),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        #### save network\n",
    "        if batch % save_frequency == 0:\n",
    "            if val_acc > best_saved_acc:\n",
    "                # save weights\n",
    "                print(\"saving weights | batch = {} | acc = {}\".format(batch, int(val_acc * 100)))\n",
    "                model.encoder.save_weights((cpt_path / \"encoder\").as_posix())\n",
    "                model.embedder.save_weights((cpt_path / \"embedder\").as_posix())\n",
    "                model.classifier.save_weights((cpt_path / \"classifier\").as_posix())\n",
    "                best_saved_acc = val_acc.numpy()\n",
    "                # save batch number\n",
    "                best_saved_batch = copy.deepcopy(batch)\n",
    "\n",
    "        if val_acc < (best_saved_acc - max_reinitialize_delta):\n",
    "            lr = model.optimizer.lr.numpy()\n",
    "            # reload weights\n",
    "            print(\n",
    "                \"batch {} | acc = {} | learn rate = {} | reloading weights from batch {} at acc {} | n reloads = {}\".format(\n",
    "                    batch, str(round(val_acc.numpy(),3)), lr, best_saved_batch, int(best_saved_acc * 100), n_reloads\n",
    "                )\n",
    "            )\n",
    "\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "\n",
    "            model.encoder.load_weights((cpt_path / \"encoder\").as_posix())\n",
    "            model.embedder.load_weights((cpt_path / \"embedder\").as_posix())\n",
    "            model.classifier.load_weights((cpt_path / \"classifier\").as_posix())\n",
    "\n",
    "            # reset batch\n",
    "            batch = copy.deepcopy(best_saved_batch)\n",
    "            # reset optimizer\n",
    "            for var in model.optimizer.variables():\n",
    "                var.assign(tf.zeros_like(var))\n",
    "\n",
    "            # set new learning rate\n",
    "            model.optimizer.lr.assign(lr / 2)\n",
    "\n",
    "            batch += 1\n",
    "            n_reloads += 1\n",
    "\n",
    "            # stop if reloaded too many times\n",
    "            if n_reloads >= max_reloads:\n",
    "                break\n",
    "                early_stopping = True\n",
    "\n",
    "            model.class_acc.reset_states()\n",
    "            model.class_acc_val.reset_states()\n",
    "            # continue on with newly updated batch (past early stopping)\n",
    "            continue\n",
    "\n",
    "        #### early stopping\n",
    "        # if there is an imporovement, set new best score\n",
    "        if val_acc > best_acc + min_delta:\n",
    "            last_improvement = 0\n",
    "            best_acc = val_acc\n",
    "        else:\n",
    "            # if model has not improved and patience has been surpassed, quit\n",
    "            if last_improvement >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                early_stopping = True\n",
    "                break\n",
    "            else:\n",
    "                last_improvement += 1\n",
    "\n",
    "        batch += 1\n",
    "    if early_stopping:\n",
    "        break\n",
    "    if n_reloads >= max_reloads:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.604Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import retrieve_tensors\n",
    "import seaborn as sns\n",
    "\n",
    "from tfumap.semisupervised_plotting import embed_data\n",
    "\n",
    "loss_df = retrieve_tensors(model.tensorboard_logdir)\n",
    "loss_df['step'] +=1\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.606Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_df.to_pickle(cpt_path / 'loss_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.607Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_acc = loss_df[\n",
    "    (loss_df.group.values == \"valid\") & (loss_df.variable.values == \"classif_acc\")\n",
    "].val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.609Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.613Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = [model.classifier(model.encoder(np.expand_dims(i, 0))) for i in tqdm(X_test)]\n",
    "y_pred = np.vstack(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.615Z"
    }
   },
   "outputs": [],
   "source": [
    "test_acc = tf.keras.metrics.sparse_categorical_accuracy(\n",
    "    Y_test, y_pred\n",
    ").numpy()\n",
    "print(np.mean(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-31T16:57:15.616Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save(cpt_path / 'test_acc.npy', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

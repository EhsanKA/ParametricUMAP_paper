{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:22.931389Z",
     "start_time": "2020-07-29T03:39:22.914230Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:22.941961Z",
     "start_time": "2020-07-29T03:39:22.933455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:26.642313Z",
     "start_time": "2020-07-29T03:39:22.943107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:26.683047Z",
     "start_time": "2020-07-29T03:39:26.643947Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:26.716316Z",
     "start_time": "2020-07-29T03:39:26.685239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020_07_28_20_39_26_714283\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "dataset = 'fmnist'\n",
    "dims = (28,28,1)\n",
    "umap_prop = 0.0\n",
    "num_classes = 10\n",
    "PROJECTION_DIMS = 128\n",
    "labels_per_class = 'full'\n",
    "datestring = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S_%f\")\n",
    "print(datestring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:31.370146Z",
     "start_time": "2020-07-29T03:39:26.717856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import umap\n",
    "import copy\n",
    "import os, tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:31.455317Z",
     "start_time": "2020-07-29T03:39:31.371744Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.load_datasets import load_FMNIST, mask_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:31.978583Z",
     "start_time": "2020-07-29T03:39:31.456555Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_FMNIST(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:32.020568Z",
     "start_time": "2020-07-29T03:39:31.979788Z"
    }
   },
   "outputs": [],
   "source": [
    "if labels_per_class == \"full\":\n",
    "    X_labeled = X_train\n",
    "    Y_labeled = Y_train\n",
    "    Y_masked = np.ones(len(Y_train))\n",
    "else:\n",
    "    X_labeled, Y_labeled, Y_masked = mask_labels(\n",
    "        X_train, Y_train, labels_per_class=labels_per_class\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build umap graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:32.067114Z",
     "start_time": "2020-07-29T03:39:32.021755Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised import build_fuzzy_simplicial_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:51.858746Z",
     "start_time": "2020-07-29T03:39:32.069117Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neighbors = 15  # default = 15\n",
    "umap_graph = build_fuzzy_simplicial_set(\n",
    "    X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:]))),\n",
    "    y=Y_masked,\n",
    "    n_neighbors=n_neighbors,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:51.900078Z",
     "start_time": "2020-07-29T03:39:51.860307Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import compute_cross_entropy, get_graph_elements\n",
    "from tfumap.semisupervised import create_edge_iterator, create_validation_iterator, create_classification_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:52.082842Z",
     "start_time": "2020-07-29T03:39:51.901390Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 200\n",
    "graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "            umap_graph, n_epochs\n",
    ")\n",
    "batch_size = np.min([n_vertices, 1000])\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:52.705170Z",
     "start_time": "2020-07-29T03:39:52.084219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# make sure batch size is no bigger than the number of labels per class\n",
    "labeled_batch_size = batch_size if batch_size < len(Y_labeled) else len(Y_labeled)\n",
    "labeled_iter = create_classification_iterator(X_labeled, Y_labeled, batch_size=batch_size)\n",
    "print(labeled_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.214291Z",
     "start_time": "2020-07-29T03:39:52.707978Z"
    }
   },
   "outputs": [],
   "source": [
    "max_sample_repeats_per_epoch = 25\n",
    "edge_iter, n_edges_per_epoch = create_edge_iterator(\n",
    "    head,\n",
    "    tail,\n",
    "    weight,\n",
    "    batch_size=batch_size,\n",
    "    max_sample_repeats_per_epoch=max_sample_repeats_per_epoch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.259205Z",
     "start_time": "2020-07-29T03:39:56.215605Z"
    }
   },
   "outputs": [],
   "source": [
    "data_valid, n_valid_samp = create_validation_iterator(X_valid, Y_valid, batch_size, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.668741Z",
     "start_time": "2020-07-29T03:39:56.260448Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.Sequential()\n",
    "\n",
    "encoder.add(tf.keras.layers.InputLayer(input_shape=dims))\n",
    "\n",
    "# Conv + Maxpooling\n",
    "encoder.add(tf.keras.layers.Convolution2D(64, (4, 4), padding='same', activation='relu'))\n",
    "encoder.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Dropout\n",
    "encoder.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "# Conv + Maxpooling\n",
    "encoder.add(tf.keras.layers.Convolution2D(64, (4, 4), activation='relu'))\n",
    "encoder.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Dropout\n",
    "encoder.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "# Converting 3D feature to 1D feature Vektor\n",
    "encoder.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Fully Connected Layer\n",
    "encoder.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "\n",
    "# Dropout\n",
    "encoder.add(tf.keras.layers.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.724030Z",
     "start_time": "2020-07-29T03:39:56.670163Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential()\n",
    "classifier.add(tf.keras.layers.InputLayer(input_shape=256))\n",
    "# Normalization\n",
    "classifier.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dense(num_classes, activation='softmax', name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.776805Z",
     "start_time": "2020-07-29T03:39:56.725386Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = tf.keras.Sequential()\n",
    "embedder.add(tf.keras.Input(shape=(256)))\n",
    "embedder.add(tf.keras.layers.Dense(units=64, activation=\"relu\"))\n",
    "embedder.add(tf.keras.layers.Dense(PROJECTION_DIMS, activation=None, name='z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.821321Z",
     "start_time": "2020-07-29T03:39:56.779705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 64)        1088      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        65600     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               409856    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "=================================================================\n",
      "Total params: 476,544\n",
      "Trainable params: 476,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create UMAP object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.860401Z",
     "start_time": "2020-07-29T03:39:56.823013Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_model import PUMAP, compute_classifier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.897220Z",
     "start_time": "2020-07-29T03:39:56.861670Z"
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.global_variables_initializer()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:56.981954Z",
     "start_time": "2020-07-29T03:39:56.898516Z"
    }
   },
   "outputs": [],
   "source": [
    "### Initialize model\n",
    "model = PUMAP(\n",
    "    min_dist = 0.0,\n",
    "    negative_sample_rate = 5, # how many negative samples per positive\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3), # cross-entropy optimizer\n",
    "    encoder=encoder,\n",
    "    embedder=embedder,\n",
    "    classifier=classifier,\n",
    "    umap_prop=umap_prop\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.662420Z",
     "start_time": "2020-07-29T03:39:22.931Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.semisupervised_plotting import plot_umap_classif_results, plot_results, get_decision_contour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.662902Z",
     "start_time": "2020-07-29T03:39:22.933Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_umap_classif_results(model, X_valid, Y_valid, X_train, X_labeled, Y_labeled, batch_size, cmap='tab10', cmap2='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.663441Z",
     "start_time": "2020-07-29T03:39:22.935Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = 0; epoch = 0\n",
    "N_EPOCHS = 10\n",
    "BATCHES_PER_EPOCH = int(n_edges_per_epoch / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:44:08.955247Z",
     "start_time": "2020-07-29T03:44:08.396394Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N_EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-95aae4ef00a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0medge_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     for (batch_to, batch_from), (X, y), (X_val, y_val) in tqdm(\n\u001b[1;32m      3\u001b[0m         \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabeled_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCHES_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     ):\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'N_EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "for edge_epoch, epoch in tqdm(zip(edge_iter, np.arange(N_EPOCHS)), total=N_EPOCHS):\n",
    "    for (batch_to, batch_from), (X, y), (X_val, y_val) in tqdm(\n",
    "        zip(edge_epoch, labeled_iter, data_valid), total=BATCHES_PER_EPOCH, leave=False\n",
    "    ):\n",
    "\n",
    "        # train\n",
    "        (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            umap_loss,\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        ) = model.train(\n",
    "            batch_to=X_train[batch_to], batch_from=X_train[batch_from], X=X, y=y\n",
    "        )\n",
    "\n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = compute_classifier_loss(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            model.encoder,\n",
    "            model.classifier,\n",
    "            model.sparse_ce,\n",
    "            model.class_acc_val,\n",
    "        )\n",
    "\n",
    "        # save losses\n",
    "        model.write_losses(\n",
    "            tf.convert_to_tensor(batch, dtype=tf.int64),\n",
    "            classifier_acc,\n",
    "            classifier_loss,\n",
    "            umap_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "        )\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "            print(\n",
    "                \"train acc: {} | val acc: {}\".format(\n",
    "                    str(round(classifier_acc.numpy(), 4)),\n",
    "                    str(round(val_acc.numpy(), 4)),\n",
    "                )\n",
    "            )\n",
    "        batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize model\n",
    "model = PUMAP(\n",
    "    min_dist = 0.0,\n",
    "    negative_sample_rate = 5, # how many negative samples per positive\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3), # cross-entropy optimizer\n",
    "    encoder=encoder,\n",
    "    embedder=embedder,\n",
    "    classifier=classifier,\n",
    "    umap_prop=umap_prop\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.664489Z",
     "start_time": "2020-07-29T03:39:22.937Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier_acc, classifier_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.665125Z",
     "start_time": "2020-07-29T03:39:22.939Z"
    }
   },
   "outputs": [],
   "source": [
    "val_acc, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.665597Z",
     "start_time": "2020-07-29T03:39:22.940Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.umap import retrieve_tensors\n",
    "import seaborn as sns\n",
    "\n",
    "from tfumap.semisupervised_plotting import embed_data\n",
    "\n",
    "loss_df = retrieve_tensors(model.tensorboard_logdir)\n",
    "loss_df['step'] +=1\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.666104Z",
     "start_time": "2020-07-29T03:39:22.941Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_acc = loss_df[\n",
    "    (loss_df.group.values == \"valid\") & (loss_df.variable.values == \"classif_acc\")\n",
    "].val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.666614Z",
     "start_time": "2020-07-29T03:39:22.943Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.667127Z",
     "start_time": "2020-07-29T03:39:22.944Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(valid_acc[10000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.667605Z",
     "start_time": "2020-07-29T03:39:22.945Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.gradient(valid_acc)[10000:][::1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.668121Z",
     "start_time": "2020-07-29T03:39:22.947Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.gradient(valid_acc)[20000:22000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.668618Z",
     "start_time": "2020-07-29T03:39:22.949Z"
    }
   },
   "outputs": [],
   "source": [
    "# if accuracy on validation is no longer incrasing by at least min_delta, for the past patience batches, stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.669190Z",
     "start_time": "2020-07-29T03:39:22.950Z"
    }
   },
   "outputs": [],
   "source": [
    "patience = 0\n",
    "min_delta = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/43906048/which-parameters-should-be-used-for-early-stopping\n",
    "\n",
    "after check_interval batches\n",
    "save model\n",
    "save train acc, val acc\n",
    "if loss does not go down by more than min_delta, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.669721Z",
     "start_time": "2020-07-29T03:39:22.952Z"
    }
   },
   "outputs": [],
   "source": [
    "patience = 10000 # wait this many batches without improvement before early stopping\n",
    "min_delta = 0.0001 # threshold for what counts as an improvement\n",
    "best_acc = 0 # the best current accuracy score\n",
    "last_improvement = 0 # delta between current batch, and the last batch that was an improvement\n",
    "for batch in range(len(valid_acc)):\n",
    "    val_acc = valid_acc[batch]\n",
    "    if val_acc > best_acc + min_delta:\n",
    "        last_improvement = 0\n",
    "        best_acc = val_acc\n",
    "    else:\n",
    "        if last_improvement>= patience:\n",
    "            break\n",
    "        else:\n",
    "            last_improvement+=1\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.670286Z",
     "start_time": "2020-07-29T03:39:22.953Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(valid_acc)\n",
    "ax.axvline(batch, color='red')\n",
    "ax.set_ylim([.91, .914])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.670894Z",
     "start_time": "2020-07-29T03:39:22.955Z"
    }
   },
   "outputs": [],
   "source": [
    "# early stopping parameters\n",
    "patience = 10000 # wait this many batches without improvement before early stopping\n",
    "min_delta = 0.0001 # threshold for what counts as an improvement\n",
    "best_acc = 0 # the best current accuracy score\n",
    "last_improvement = 0 # delta between current batch, and the last batch that was an improvement\n",
    "best_saved_acc = 0 # best accuracy on valid data that has been checkpointed\n",
    "best_saved_batch = 0 # batch number for last good batch\n",
    "max_reinitialize_delta = .01 # minimum loss in accuracy resulting in reinitialized weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.671400Z",
     "start_time": "2020-07-29T03:39:22.957Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_frequency = 25000\n",
    "save_frequency = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.671913Z",
     "start_time": "2020-07-29T03:39:22.958Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.paths import MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.672421Z",
     "start_time": "2020-07-29T03:39:22.959Z"
    }
   },
   "outputs": [],
   "source": [
    "cpt_path = MODEL_DIR / 'semisupervised' / dataset / datestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.672915Z",
     "start_time": "2020-07-29T03:39:22.960Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save(cpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.673997Z",
     "start_time": "2020-07-29T03:39:22.962Z"
    }
   },
   "outputs": [],
   "source": [
    "for edge_epoch, epoch in tqdm(zip(edge_iter, np.arange(N_EPOCHS)), total=N_EPOCHS):\n",
    "    for (batch_to, batch_from), (X, y), (X_val, y_val) in tqdm(\n",
    "        zip(edge_epoch, labeled_iter, data_valid), total=BATCHES_PER_EPOCH, leave=False\n",
    "    ):\n",
    "\n",
    "        # train\n",
    "        (\n",
    "            attraction_loss,\n",
    "            repellant_loss,\n",
    "            umap_loss,\n",
    "            classifier_loss,\n",
    "            classifier_acc,\n",
    "        ) = model.train(\n",
    "            batch_to=X_train[batch_to], batch_from=X_train[batch_from], X=X, y=y\n",
    "        )\n",
    "\n",
    "        # compute validation loss\n",
    "        val_loss, val_acc = compute_classifier_loss(\n",
    "            X_val,\n",
    "            y_val,\n",
    "            model.encoder,\n",
    "            model.classifier,\n",
    "            model.sparse_ce,\n",
    "            model.class_acc_val,\n",
    "        )\n",
    "\n",
    "        # save losses\n",
    "        model.write_losses(\n",
    "            tf.convert_to_tensor(batch, dtype=tf.int64),\n",
    "            classifier_acc,\n",
    "            classifier_loss,\n",
    "            umap_loss,\n",
    "            val_loss,\n",
    "            val_acc,\n",
    "        )\n",
    "        \n",
    "        # plot results\n",
    "        if batch % plot_frequency == 0:\n",
    "            plot_umap_classif_results(\n",
    "                model,\n",
    "                X_valid,\n",
    "                Y_valid,\n",
    "                X_train,\n",
    "                X_labeled,\n",
    "                Y_labeled,\n",
    "                batch_size,\n",
    "                cmap=\"tab10\",\n",
    "                cmap2=\"tab10\",\n",
    "            )\n",
    "            print(\n",
    "                \"train acc: {} | val acc: {}\".format(\n",
    "                    str(round(classifier_acc.numpy(), 4)),\n",
    "                    str(round(val_acc.numpy(), 4)),\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # save network\n",
    "        if batch % save_frequency == 0:\n",
    "            if val_acc > best_saved_acc:\n",
    "                # save weights\n",
    "                model.encoder.save(cpt_path / str(batch) / 'encoder')\n",
    "                model.embedder.save(cpt_path / str(batch) / 'embedder')\n",
    "                model.classifier.save(cpt_path / str(batch) / 'classifier')\n",
    "                \n",
    "                # save batch number\n",
    "                best_saved_batch = copy.deepcopy(batch)\n",
    "            \n",
    "            elif val_acc < best_saved_acc - max_reinitialize_delta: \n",
    "                # reload weights\n",
    "                model.encoder.load(cpt_path / str(best_saved_batch) / 'encoder')\n",
    "                model.embedder.load(cpt_path / str(best_saved_batch) / 'embedder')\n",
    "                model.classifier.load(cpt_path / str(best_saved_batch) / 'classifier')\n",
    "                \n",
    "                # reset batch\n",
    "                batch = copy.deepcopy(best_saved_batch)\n",
    "                # reset optimizer\n",
    "                for var in model.optimizer.variables():\n",
    "                    var.assign(tf.zeros_like(var))\n",
    "                    \n",
    "                # continue on with newly updated batch (past early stopping)\n",
    "                continue\n",
    "        \n",
    "        #### early stopping\n",
    "        # if there is an imporovement, set new best score\n",
    "        if val_acc > best_acc + min_delta:\n",
    "            last_improvement = 0\n",
    "            best_acc = val_acc\n",
    "        else:\n",
    "            # if model has not improved and patience has been surpassed, quit\n",
    "            if last_improvement>= patience:\n",
    "                break\n",
    "            else:\n",
    "                last_improvement+=1\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "        batch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the accuracy is better than the best save accuracy - save\n",
    "# if the accuracy is worse than the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.675172Z",
     "start_time": "2020-07-29T03:39:22.963Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-29T03:39:58.676270Z",
     "start_time": "2020-07-29T03:39:22.965Z"
    }
   },
   "outputs": [],
   "source": [
    "# save every n batches\n",
    "# if accuracy goes down from best accuracy by more than max_reinitialize_delta, reload lasta saved weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

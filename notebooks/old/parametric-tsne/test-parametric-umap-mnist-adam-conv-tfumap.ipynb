{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:45.708790Z",
     "start_time": "2020-07-20T06:03:45.706785Z"
    }
   },
   "outputs": [],
   "source": [
    "### based on https://github.com/kylemcdonald/Parametric-t-SNE/blob/master/Parametric%20t-SNE%20(Keras).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:45.753469Z",
     "start_time": "2020-07-20T06:03:45.710218Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU (this may not be needed on your computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:45.773472Z",
     "start_time": "2020-07-20T06:03:45.754727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:49.372632Z",
     "start_time": "2020-07-20T06:03:45.775019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:49.448949Z",
     "start_time": "2020-07-20T06:03:49.373812Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:49.579541Z",
     "start_time": "2020-07-20T06:03:49.450657Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:49.626076Z",
     "start_time": "2020-07-20T06:03:49.580870Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.parametric_tsne import compute_joint_probabilities, tsne_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:49.665304Z",
     "start_time": "2020-07-20T06:03:49.627155Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = 'mnist'\n",
    "dims = (28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:50.081953Z",
     "start_time": "2020-07-20T06:03:49.666336Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load dataset\n",
    "(train_images, Y_train), (test_images, Y_test) = mnist.load_data()\n",
    "X_train = (train_images / 255.0).astype(\"float32\")\n",
    "X_test = (test_images / 255.0).astype(\"float32\")\n",
    "X_train = np.expand_dims(X_train, -1)#((len(X_train), np.product(np.shape(X_train)[1:])))\n",
    "#X_train = X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:])))\n",
    "#X_test = X_test.reshape((len(X_test), np.product(np.shape(X_test)[1:])))\n",
    "\n",
    "# subset a validation set\n",
    "n_valid = 10000\n",
    "X_valid = X_train[-n_valid:]\n",
    "Y_valid = Y_train[-n_valid:]\n",
    "X_train = X_train[:-n_valid]\n",
    "Y_train = Y_train[:-n_valid]\n",
    "\n",
    "# flatten X\n",
    "X_train_flat = X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:])))\n",
    "X_test_flat = X_test.reshape((len(X_test), np.product(np.shape(X_test)[1:])))\n",
    "X_valid_flat = X_valid.reshape((len(X_valid), np.product(np.shape(X_valid)[1:])))\n",
    "X_test = X_test.reshape((10000, 28, 28, 1))\n",
    "print(len(X_train), len(X_valid), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:50.120596Z",
     "start_time": "2020-07-20T06:03:50.083002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:03:50.158445Z",
     "start_time": "2020-07-20T06:03:50.121653Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:41.936250Z",
     "start_time": "2020-07-20T06:03:50.159419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing P-values...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffca1e3bccf64b6ebe273f5a1897d56f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.210395926597027\n",
      "Minimum value of sigma: 1.1465189822400297\n",
      "Maximum value of sigma: 3.423562241637291\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2133320422641924\n",
      "Minimum value of sigma: 1.028104158803783\n",
      "Maximum value of sigma: 3.3874801273632515\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.254363937312274\n",
      "Minimum value of sigma: 1.0993403688963808\n",
      "Maximum value of sigma: 3.6720262251663818\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2162078644773335\n",
      "Minimum value of sigma: 1.081392000059354\n",
      "Maximum value of sigma: 3.3301979665436727\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.247200240446076\n",
      "Minimum value of sigma: 1.061683289843211\n",
      "Maximum value of sigma: 3.476759114723516\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.181038068415593\n",
      "Minimum value of sigma: 0.9031738375276179\n",
      "Maximum value of sigma: 3.665544901255984\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.1962136441831337\n",
      "Minimum value of sigma: 1.0022621389059558\n",
      "Maximum value of sigma: 3.420674280853297\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.216737423713201\n",
      "Minimum value of sigma: 1.0503807230236393\n",
      "Maximum value of sigma: 3.3508994412507405\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.200239991624926\n",
      "Minimum value of sigma: 1.1353135977722875\n",
      "Maximum value of sigma: 3.3820576890069067\n",
      "Computing pairwise distances...\n",
      "Computing P-values...\n",
      "Computed P-values 0 of 5000 datapoints...\n",
      "Computed P-values 500 of 5000 datapoints...\n",
      "Computed P-values 1000 of 5000 datapoints...\n",
      "Computed P-values 1500 of 5000 datapoints...\n",
      "Computed P-values 2000 of 5000 datapoints...\n",
      "Computed P-values 2500 of 5000 datapoints...\n",
      "Computed P-values 3000 of 5000 datapoints...\n",
      "Computed P-values 3500 of 5000 datapoints...\n",
      "Computed P-values 4000 of 5000 datapoints...\n",
      "Computed P-values 4500 of 5000 datapoints...\n",
      "Mean value of sigma: 2.2134461707301147\n",
      "Minimum value of sigma: 1.0805969136944074\n",
      "Maximum value of sigma: 3.3211063109107593\n",
      "\n"
     ]
    }
   ],
   "source": [
    "P = compute_joint_probabilities(X_train_flat, batch_size=batch_size, perplexity=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:42.397912Z",
     "start_time": "2020-07-20T06:04:41.937531Z"
    }
   },
   "outputs": [],
   "source": [
    "from tfumap.paths import ensure_dir, MODEL_DIR, DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:42.435196Z",
     "start_time": "2020-07-20T06:04:42.399596Z"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:43.732579Z",
     "start_time": "2020-07-20T06:04:42.436869Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:43.767393Z",
     "start_time": "2020-07-20T06:04:43.733794Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.114414Z",
     "start_time": "2020-07-20T06:04:43.768466Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-92270448c7af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtsne_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m       \u001b[0;31m# Creates the model loss and weighted metrics sub-graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_weights_loss_and_weighted_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_weights_loss_and_weighted_metrics\u001b[0;34m(self, sample_weights)\u001b[0m\n\u001b[1;32m   1590\u001b[0m       \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reduction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mper_sample_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m             weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    220\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/umap_tf_networks/tfumap/parametric_tsne.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(P, Z)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeras\u001b[0m \u001b[0mloss\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0mcall\u001b[0m \u001b[0mthis\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz2p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/umap_tf_networks/tfumap/parametric_tsne.py\u001b[0m in \u001b[0;36mz2p\u001b[0;34m(z, n, eps)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \"\"\" Computes the low dimensional probability\n\u001b[1;32m    139\u001b[0m     \"\"\"\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0msum_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_act\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msum_act\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tsne_loss(d=n_components, batch_size=batch_size), optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.115831Z",
     "start_time": "2020-07-20T06:03:45.688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Joint probabilities of data\n",
    "Y_train_tsne = P.reshape(X_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.116406Z",
     "start_time": "2020-07-20T06:03:45.689Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_train_tsne.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.116964Z",
     "start_time": "2020-07-20T06:03:45.694Z"
    }
   },
   "outputs": [],
   "source": [
    "# because shuffle == False, the same batches are used each time...\n",
    "history = model.fit(X_train, Y_train_tsne, batch_size=batch_size, shuffle=False, nb_epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.117462Z",
     "start_time": "2020-07-20T06:03:45.695Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.117969Z",
     "start_time": "2020-07-20T06:03:45.696Z"
    }
   },
   "outputs": [],
   "source": [
    "z = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-20T06:04:44.118543Z",
     "start_time": "2020-07-20T06:03:45.697Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.scatter(z[:, 0], z[:, 1], s=0.1, alpha=0.5, c=Y_train, cmap=plt.cm.tab10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

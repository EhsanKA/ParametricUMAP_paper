{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- passing in callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:32.323061Z",
     "start_time": "2020-08-17T00:14:32.265058Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:32.333902Z",
     "start_time": "2020-08-17T00:14:32.324506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:36.244162Z",
     "start_time": "2020-08-17T00:14:32.335116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:37.199963Z",
     "start_time": "2020-08-17T00:14:36.245834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tfumap.load_datasets import load_MNIST, mask_labels\n",
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_MNIST(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.335655Z",
     "start_time": "2020-08-17T00:14:37.201991Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from umap import UMAP\n",
    "from warnings import warn, catch_warnings, filterwarnings\n",
    "from umap.umap_ import make_epochs_per_sample\n",
    "from numba import TypingError\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.379124Z",
     "start_time": "2020-08-17T00:14:41.337678Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_graph_elements(graph_, n_epochs):\n",
    "    \"\"\"\n",
    "    gets elements of graphs, weights, and number of epochs per edge\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_ : scipy.sparse.csr.csr_matrix\n",
    "        umap graph of probabilities\n",
    "    n_epochs : int\n",
    "        maximum number of epochs per edge\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    graph scipy.sparse.csr.csr_matrix\n",
    "        umap graph\n",
    "    epochs_per_sample np.array\n",
    "        number of epochs to train each sample for\n",
    "    head np.array\n",
    "        edge head\n",
    "    tail np.array\n",
    "        edge tail\n",
    "    weight np.array\n",
    "        edge weight\n",
    "    n_vertices int\n",
    "        number of verticies in graph\n",
    "    \"\"\"\n",
    "    ### should we remove redundancies () here??\n",
    "    # graph_ = remove_redundant_edges(graph_)\n",
    "\n",
    "    graph = graph_.tocoo()\n",
    "    # eliminate duplicate entries by summing them together\n",
    "    graph.sum_duplicates()\n",
    "    # number of vertices in dataset\n",
    "    n_vertices = graph.shape[1]\n",
    "    # get the number of epochs based on the size of the dataset\n",
    "    if n_epochs is None:\n",
    "        # For smaller datasets we can use more epochs\n",
    "        if graph.shape[0] <= 10000:\n",
    "            n_epochs = 500\n",
    "        else:\n",
    "            n_epochs = 200\n",
    "    # remove elements with very low probability\n",
    "    graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "    graph.eliminate_zeros()\n",
    "    # get epochs per sample based upon edge probability\n",
    "    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "    head = graph.row\n",
    "    tail = graph.col\n",
    "    weight = graph.data\n",
    "\n",
    "    return graph, epochs_per_sample, head, tail, weight, n_vertices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.424417Z",
     "start_time": "2020-08-17T00:14:41.381188Z"
    }
   },
   "outputs": [],
   "source": [
    "from umap.spectral import spectral_layout\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "\n",
    "def init_embedding_from_graph(\n",
    "    _raw_data, graph, n_components, random_state, metric, _metric_kwds, init=\"spectral\"\n",
    "):\n",
    "    \"\"\" Initialize embedding using graph. This is for direct embeddings. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        init : str, optional\n",
    "            Type of initialization to use. Either random, or spectral, by default \"spectral\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedding : np.array\n",
    "            the initialized embedding\n",
    "        \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = check_random_state(None)\n",
    "\n",
    "    if isinstance(init, str) and init == \"random\":\n",
    "        embedding = random_state.uniform(\n",
    "            low=-10.0, high=10.0, size=(graph.shape[0], n_components)\n",
    "        ).astype(np.float32)\n",
    "    elif isinstance(init, str) and init == \"spectral\":\n",
    "        # We add a little noise to avoid local minima for optimization to come\n",
    "\n",
    "        initialisation = spectral_layout(\n",
    "            _raw_data,\n",
    "            graph,\n",
    "            n_components,\n",
    "            random_state,\n",
    "            metric=metric,\n",
    "            metric_kwds=_metric_kwds,\n",
    "        )\n",
    "        expansion = 10.0 / np.abs(initialisation).max()\n",
    "        embedding = (initialisation * expansion).astype(\n",
    "            np.float32\n",
    "        ) + random_state.normal(\n",
    "            scale=0.0001, size=[graph.shape[0], n_components]\n",
    "        ).astype(\n",
    "            np.float32\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        init_data = np.array(init)\n",
    "        if len(init_data.shape) == 2:\n",
    "            if np.unique(init_data, axis=0).shape[0] < init_data.shape[0]:\n",
    "                tree = KDTree(init_data)\n",
    "                dist, ind = tree.query(init_data, k=2)\n",
    "                nndist = np.mean(dist[:, 1])\n",
    "                embedding = init_data + random_state.normal(\n",
    "                    scale=0.001 * nndist, size=init_data.shape\n",
    "                ).astype(np.float32)\n",
    "            else:\n",
    "                embedding = init_data\n",
    "\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.456442Z",
     "start_time": "2020-08-17T00:14:41.425773Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_distance_to_probability(distances, a, b):\n",
    "    \"\"\" convert distance representation into probability, \n",
    "        as a function of a, b params\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + a * distances ** (2 * b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.488367Z",
     "start_time": "2020-08-17T00:14:41.457573Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cross_entropy(\n",
    "    probabilities_graph, probabilities_distance, EPS=1e-4, repulsion_strength=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute cross entropy between low and high probability\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probabilities_graph : array\n",
    "        high dimensional probabilities\n",
    "    probabilities_distance : array\n",
    "        low dimensional probabilities\n",
    "    EPS : float, optional\n",
    "        offset to to ensure log is taken of a positive number, by default 1e-4\n",
    "    repulsion_strength : float, optional\n",
    "        strength of repulsion between negative samples, by default 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    attraction_term: tf.float32\n",
    "        attraction term for cross entropy loss\n",
    "    repellant_term: tf.float32\n",
    "        repellant term for cross entropy loss\n",
    "    cross_entropy: tf.float32\n",
    "        cross entropy umap loss\n",
    "    \n",
    "    \"\"\"\n",
    "    # cross entropy\n",
    "    attraction_term = -probabilities_graph * tf.math.log(\n",
    "        tf.clip_by_value(probabilities_distance, EPS, 1.0)\n",
    "    )\n",
    "    repellant_term = (\n",
    "        -(1.0 - probabilities_graph)\n",
    "        * tf.math.log(tf.clip_by_value(1.0 - probabilities_distance, EPS, 1.0))\n",
    "        * repulsion_strength\n",
    "    )\n",
    "\n",
    "    # balance the expected losses between atrraction and repel\n",
    "    CE = attraction_term + repellant_term\n",
    "    return attraction_term, repellant_term, CE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.527527Z",
     "start_time": "2020-08-17T00:14:41.490252Z"
    }
   },
   "outputs": [],
   "source": [
    "def umap_loss(\n",
    "    batch_size,\n",
    "    negative_sample_rate,\n",
    "    _a,\n",
    "    _b,\n",
    "    edge_weights,\n",
    "    parametric_embedding,\n",
    "    repulsion_strength=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a keras-ccompatible loss function for UMAP loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        size of mini-batches\n",
    "    negative_sample_rate : int\n",
    "        number of negative samples per positive samples to train on\n",
    "    _a : float\n",
    "        distance parameter in embedding space\n",
    "    _b : float float\n",
    "        distance parameter in embedding space\n",
    "    edge_weights : array\n",
    "        weights of all edges from sparse UMAP graph\n",
    "    parametric_embedding : bool\n",
    "        whether the embeddding is parametric or nonparametric\n",
    "    repulsion_strength : float, optional\n",
    "        strength of repulsion vs attraction for cross-entropy, by default 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : function\n",
    "        loss function that takes in a placeholder (0) and the output of the keras network\n",
    "    \"\"\"\n",
    "\n",
    "    if not parametric_embedding:\n",
    "        # multiply loss by weights for nonparametric\n",
    "        weights_tiled = np.tile(edge_weights, negative_sample_rate + 1)\n",
    "\n",
    "    @tf.function\n",
    "    def loss(placeholder_y, embed_to_from):\n",
    "        # split out to/from\n",
    "        embedding_to, embedding_from = tf.split(\n",
    "            embed_to_from, num_or_size_splits=2, axis=1\n",
    "        )\n",
    "\n",
    "        # get negative samples\n",
    "        embedding_neg_to = tf.repeat(embedding_to, negative_sample_rate, axis=0)\n",
    "        repeat_neg = tf.repeat(embedding_from, negative_sample_rate, axis=0)\n",
    "        embedding_neg_from = tf.gather(\n",
    "            repeat_neg, tf.random.shuffle(tf.range(tf.shape(repeat_neg)[0]))\n",
    "        )\n",
    "\n",
    "        #  distances between samples (and negative samples)\n",
    "        distance_embedding = tf.concat(\n",
    "            [\n",
    "                tf.norm(embedding_to - embedding_from, axis=1),\n",
    "                tf.norm(embedding_neg_to - embedding_neg_from, axis=1),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        # convert probabilities to distances\n",
    "        probabilities_distance = convert_distance_to_probability(\n",
    "            distance_embedding, _a, _b\n",
    "        )\n",
    "\n",
    "        # set true probabilities based on negative sampling\n",
    "        probabilities_graph = tf.concat(\n",
    "            [tf.ones(batch_size), tf.zeros(batch_size * negative_sample_rate)], axis=0,\n",
    "        )\n",
    "\n",
    "        # compute cross entropy\n",
    "        (attraction_loss, repellant_loss, ce_loss) = compute_cross_entropy(\n",
    "            probabilities_graph,\n",
    "            probabilities_distance,\n",
    "            repulsion_strength=repulsion_strength,\n",
    "        )\n",
    "\n",
    "        if not parametric_embedding:\n",
    "            ce_loss = ce_loss * weights_tiled\n",
    "\n",
    "        return tf.reduce_mean(ce_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.563620Z",
     "start_time": "2020-08-17T00:14:41.528812Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_networks(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    n_components,\n",
    "    dims,\n",
    "    n_data,\n",
    "    parametric_embedding,\n",
    "    parametric_reconstruction,\n",
    "    init_embedding=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a set of keras networks for the encoder and decoder if one has not already\n",
    "    been predefined. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder : tf.keras.Sequential\n",
    "        The encoder Keras network\n",
    "    decoder : tf.keras.Sequential\n",
    "        the decoder Keras network\n",
    "    n_components : int\n",
    "        the dimensionality of the latent space\n",
    "    dims : tuple of shape (dim1, dim2, dim3...)\n",
    "        dimensionality of data\n",
    "    n_data : number of elements in dataset\n",
    "        # of elements in training dataset\n",
    "    parametric_embedding : bool\n",
    "        Whether the embedder is parametric or non-parametric\n",
    "    parametric_reconstruction : bool\n",
    "        Whether the decoder is parametric or non-parametric\n",
    "    init_embedding : array (optional, default None)\n",
    "        The initial embedding, for nonparametric embeddings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoder: tf.keras.Sequential\n",
    "        encoder keras network\n",
    "    decoder: tf.keras.Sequential\n",
    "        decoder keras network\n",
    "    \"\"\"\n",
    "\n",
    "    if parametric_embedding:\n",
    "        if encoder is None:\n",
    "            encoder = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "                    tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=n_components, name=\"z\"),\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            n_data, n_components, input_length=1\n",
    "        )\n",
    "        embedding_layer.build(input_shape=(1,))\n",
    "        embedding_layer.set_weights([init_embedding])\n",
    "        encoder = tf.keras.Sequential([embedding_layer])\n",
    "\n",
    "    if decoder is None:\n",
    "        if parametric_reconstruction:\n",
    "            decoder = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.InputLayer(input_shape=n_components),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(\n",
    "                        units=np.product(dims), name=\"recon\", activation=None\n",
    "                    ),\n",
    "                    tf.keras.layers.Reshape(dims),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.600941Z",
     "start_time": "2020-08-17T00:14:41.564776Z"
    }
   },
   "outputs": [],
   "source": [
    "def construct_edge_dataset(\n",
    "    X, graph_, n_epochs, batch_size, parametric_embedding, parametric_reconstruction,\n",
    "):\n",
    "    \"\"\"\n",
    "    Construct a tf.data.Dataset of edges, sampled by edge weight.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        New data to be transformed.\n",
    "    graph_ : scipy.sparse.csr.csr_matrix\n",
    "        Generated UMAP graph\n",
    "    n_epochs : int\n",
    "        # of epochs to train each edge\n",
    "    batch_size : int\n",
    "        batch size\n",
    "    parametric_embedding : bool\n",
    "        Whether the embedder is parametric or non-parametric\n",
    "    parametric_reconstruction : bool\n",
    "        Whether the decoder is parametric or non-parametric\n",
    "    \"\"\"\n",
    "\n",
    "    def gather_X(edge_to, edge_from):\n",
    "        edge_to_batch = tf.gather(X, edge_to)\n",
    "        edge_from_batch = tf.gather(X, edge_from)\n",
    "        outputs = {\"umap\": 0}\n",
    "        if parametric_reconstruction:\n",
    "            # add reconstruction to iterator output\n",
    "            # edge_out = tf.concat([edge_to_batch, edge_from_batch], axis=0)\n",
    "            outputs[\"reconstruction\"] = edge_to_batch\n",
    "\n",
    "        return (edge_to_batch, edge_from_batch), outputs\n",
    "\n",
    "    def make_sham_generator():\n",
    "        \"\"\"\n",
    "        The sham generator is used to \n",
    "        \"\"\"\n",
    "\n",
    "        def sham_generator():\n",
    "            while True:\n",
    "                yield tf.zeros(1, dtype=tf.int32), tf.zeros(1, dtype=tf.int32)\n",
    "\n",
    "        return sham_generator\n",
    "\n",
    "    # get data from graph\n",
    "    graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "        graph_, n_epochs\n",
    "    )\n",
    "\n",
    "    # number of elements per batch for embedding\n",
    "    if batch_size is None:\n",
    "        # batch size can be larger if its just over embeddings\n",
    "        if parametric_embedding:\n",
    "            batch_size = np.min([n_vertices, 1000])\n",
    "        else:\n",
    "            batch_size = len(head)\n",
    "\n",
    "    edges_to_exp, edges_from_exp = (\n",
    "        np.repeat(head, epochs_per_sample.astype(\"int\")),\n",
    "        np.repeat(tail, epochs_per_sample.astype(\"int\")),\n",
    "    )\n",
    "\n",
    "    # shuffle edges\n",
    "    shuffle_mask = np.random.permutation(range(len(edges_to_exp)))\n",
    "    edges_to_exp = edges_to_exp[shuffle_mask]\n",
    "    edges_from_exp = edges_from_exp[shuffle_mask]\n",
    "\n",
    "    # create edge iterator\n",
    "    if parametric_embedding:\n",
    "        edge_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (edges_to_exp, edges_from_exp)\n",
    "        )\n",
    "        edge_dataset = edge_dataset.repeat()\n",
    "        edge_dataset = edge_dataset.shuffle(10000)\n",
    "        edge_dataset = edge_dataset.map(\n",
    "            gather_X, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        edge_dataset = edge_dataset.batch(batch_size, drop_remainder=True)\n",
    "        edge_dataset = edge_dataset.prefetch(10)\n",
    "    else:\n",
    "        # nonparametric embedding uses a sham dataset\n",
    "        gen = make_sham_generator()\n",
    "        edge_dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32))\n",
    "    return edge_dataset, batch_size, len(edges_to_exp), head, tail, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.635444Z",
     "start_time": "2020-08-17T00:14:41.602135Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_parametric_umap_model(save_location, verbose=True):\n",
    "\n",
    "    ## Loads a parametricUMAP model and its related keras models\n",
    "    \n",
    "    model_output = os.path.join(save_location, \"model.pkl\")\n",
    "    model = pickle.load((open(model_output, 'rb')))\n",
    "    if verbose:\n",
    "        print(\"Pickle of ParametricUMAP model loaded from {}\".format(model_output))\n",
    "\n",
    "    \n",
    "    # load encoder\n",
    "    encoder_output = os.path.join(save_location, \"encoder\")\n",
    "    if os.path.exists(encoder_output):\n",
    "        model.encoder = tf.keras.models.load_model(encoder_output)\n",
    "        if verbose:\n",
    "            print(\"Keras encoder model loaded from {}\".format(encoder_output))\n",
    "\n",
    "    # save decoder\n",
    "    decoder_output = os.path.join(save_location, \"decoder\")\n",
    "    if os.path.exists(decoder_output):\n",
    "        model.decoder = tf.keras.models.load_model(decoder_output)\n",
    "        print(\"Keras decoder model loaded from {}\".format(decoder_output))\n",
    "\n",
    "        \n",
    "    # get the custom loss function\n",
    "    umap_loss_fn = umap_loss(\n",
    "            model.batch_size,\n",
    "            model.negative_sample_rate,\n",
    "            model._a,\n",
    "            model._b,\n",
    "            model.edge_weight,\n",
    "            model.parametric_embedding,\n",
    "    )\n",
    "        \n",
    "    # save parametric_model\n",
    "    parametric_model_output = os.path.join(save_location, \"parametric_model\")\n",
    "    if os.path.exists(parametric_model_output):\n",
    "        model.parametric_model = tf.keras.models.load_model(parametric_model_output, \n",
    "                                                           custom_objects={'loss': umap_loss_fn})\n",
    "        print(\"Keras full model loaded from {}\".format(parametric_model_output))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.667750Z",
     "start_time": "2020-08-17T00:14:41.636646Z"
    }
   },
   "outputs": [],
   "source": [
    "import codecs, pickle\n",
    "def should_pickle(key, val):\n",
    "    \"\"\"\n",
    "    Checks if a dictionary item can be pickled\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : try\n",
    "        key for dictionary element\n",
    "    val : None\n",
    "        element of dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    picklable: bool\n",
    "        whether the dictionary item can be pickled\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ## make sure object can be pickled and then re-read\n",
    "        # pickle object\n",
    "        pickled = codecs.encode(pickle.dumps(val), \"base64\").decode()\n",
    "        # unpickle object\n",
    "        unpickled = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n",
    "    except (\n",
    "        pickle.PicklingError,\n",
    "        tf.errors.InvalidArgumentError,\n",
    "        TypeError,\n",
    "        tf.errors.InternalError,\n",
    "        OverflowError,\n",
    "        TypingError,\n",
    "    ) as e:\n",
    "        warn(\"Did not pickle {}: {}\".format(key, e))\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.729274Z",
     "start_time": "2020-08-17T00:14:41.669116Z"
    }
   },
   "outputs": [],
   "source": [
    "class parametricUMAP(UMAP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer=None,\n",
    "        batch_size=None,\n",
    "        dims=None,\n",
    "        encoder=None,\n",
    "        decoder=None,\n",
    "        parametric_embedding=True,\n",
    "        parametric_reconstruction=False,\n",
    "        autoencoder_loss=False,\n",
    "        reconstruction_validation=None,\n",
    "        loss_report_frequency=10,\n",
    "        n_training_epochs=1,\n",
    "        keras_fit_kwargs={},\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parametric UMAP subclassing UMAP-learn, based on keras/tensorflow.\n",
    "        There is also a non-parametric implementation contained within to compare \n",
    "        with the base non-parametric implementation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : tf.keras.optimizers, optional\n",
    "            The tensorflow optimizer used for embedding, by default None\n",
    "        batch_size : int, optional\n",
    "            size of batch used for batch training, by default None\n",
    "        dims :  tuple, optional\n",
    "            dimensionality of data, if not flat (e.g. (32x32x3 images for ConvNet), by default None\n",
    "        encoder : tf.keras.Sequential, optional\n",
    "            The encoder Keras network\n",
    "        decoder : tf.keras.Sequential, optional\n",
    "            the decoder Keras network\n",
    "        parametric_embedding : bool, optional\n",
    "            Whether the embedder is parametric or non-parametric, by default True\n",
    "        parametric_reconstruction : bool, optional\n",
    "            Whether the decoder is parametric or non-parametric, by default False\n",
    "        autoencoder_loss : bool, optional\n",
    "            [description], by default False\n",
    "        reconstruction_validation : array, optional\n",
    "            validation X data for reconstruction loss, by default None\n",
    "        loss_report_frequency : int, optional\n",
    "            how many times per epoch to report loss, by default 1\n",
    "        n_training_epochs : int, optional\n",
    "            [description], by default 1\n",
    "        keras_fit_kwargs : dict, optional\n",
    "            additional arguments for model.fit (like callbacks), by default {}\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # add to network\n",
    "        self.dims = dims  # if this is an image, we should reshape for network\n",
    "        self.encoder = encoder  # neural network used for embedding\n",
    "        self.decoder = decoder  # neural network used for decoding\n",
    "        self.parametric_embedding = (\n",
    "            parametric_embedding  # nonparametric vs parametric embedding\n",
    "        )\n",
    "        self.parametric_reconstruction = parametric_reconstruction\n",
    "        self.autoencoder_loss = autoencoder_loss\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_report_frequency = (\n",
    "            loss_report_frequency  # how many times per epoch to report loss in keras\n",
    "        )\n",
    "        self.reconstruction_validation = (\n",
    "            reconstruction_validation  # holdout data for reconstruction acc\n",
    "        )\n",
    "        self.keras_fit_kwargs = keras_fit_kwargs  # arguments for model.fit\n",
    "        self.parametric_model = None\n",
    "\n",
    "        # how many epochs to train for (different than n_epochs which is specific to each sample)\n",
    "        self.n_training_epochs = n_training_epochs\n",
    "        # set optimizer\n",
    "        if optimizer is None:\n",
    "            if parametric_embedding:\n",
    "                # Adam is better for parametric_embedding\n",
    "                self.optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "            else:\n",
    "                # Larger learning rate can be used for embedding\n",
    "                self.optimizer = tf.keras.optimizers.Adam(1e-1)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        if parametric_reconstruction and not parametric_embedding:\n",
    "            warn(\n",
    "                \"Parametric decoding is not implemented with nonparametric \\\n",
    "            embedding. Turning off parametric decoding\"\n",
    "            )\n",
    "            self.parametric_reconstruction = False\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X into the existing embedded space and return that\n",
    "        transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            New data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Embedding of the new data in low-dimensional space.\n",
    "        \"\"\"\n",
    "        if self.parametric_embedding:\n",
    "            return self.encoder.predict(\n",
    "                X, batch_size=self.batch_size, verbose=self.verbose\n",
    "            )\n",
    "        else:\n",
    "            warn(\n",
    "                \"Embedding new data is not supported by parametricUMAP. \\\n",
    "                Using original embedder.\"\n",
    "            )\n",
    "            return super().transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\"Transform X in the existing embedded space back into the input\n",
    "        data space and return that transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_components)\n",
    "            New points to be inverse transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_features)\n",
    "            Generated data points new data in data space.\n",
    "        \"\"\"\n",
    "        if self.parametric_reconstruction:\n",
    "            return self.decoder.predict(\n",
    "                X, batch_size=self.batch_size, verbose=self.verbose\n",
    "            )\n",
    "        else:\n",
    "            return super().inverse_transform(X)\n",
    "\n",
    "    def _define_model(self):\n",
    "        \"\"\" Define the model in keras\n",
    "        \"\"\"\n",
    "\n",
    "        # network outputs\n",
    "        outputs = {}\n",
    "\n",
    "        # inputs\n",
    "        if self.parametric_embedding:\n",
    "            to_x = tf.keras.layers.Input(shape=self.dims, name=\"to_x\")\n",
    "            from_x = tf.keras.layers.Input(shape=self.dims, name=\"from_x\")\n",
    "            inputs = [to_x, from_x]\n",
    "\n",
    "            # parametric embedding\n",
    "            embedding_to = self.encoder(to_x)\n",
    "            embedding_from = self.encoder(from_x)\n",
    "\n",
    "            if self.parametric_reconstruction:\n",
    "                # parametric reconstruction\n",
    "                if self.autoencoder_loss:\n",
    "                    embedding_to_recon = self.decoder(embedding_to)\n",
    "                else:\n",
    "                    # stop gradient of reconstruction loss before it reaches the encoder\n",
    "                    embedding_to_recon = self.decoder(tf.stop_gradient(embedding_to))\n",
    "\n",
    "                embedding_to_recon = tf.keras.layers.Lambda(\n",
    "                    lambda x: x, name=\"reconstruction\"\n",
    "                )(embedding_to_recon)\n",
    "\n",
    "                outputs[\"reconstruction\"] = embedding_to_recon\n",
    "\n",
    "        else:\n",
    "            # this is the sham input (its just a 0) to make keras think there is input data\n",
    "            batch_sample = tf.keras.layers.Input(\n",
    "                shape=(1), dtype=tf.int32, name=\"batch_sample\"\n",
    "            )\n",
    "\n",
    "            # gather all of the edges (so keras model is happy)\n",
    "            to_x = tf.squeeze(tf.gather(self.head, batch_sample[0]))\n",
    "            from_x = tf.squeeze(tf.gather(self.tail, batch_sample[0]))\n",
    "\n",
    "            # grab relevant embeddings\n",
    "            embedding_to = self.encoder(to_x)[:, -1, :]\n",
    "            embedding_from = self.encoder(from_x)[:, -1, :]\n",
    "\n",
    "            inputs = [batch_sample]\n",
    "\n",
    "        # concatenate to/from projections for loss computation\n",
    "        embedding_to_from = tf.concat([embedding_to, embedding_from], axis=1)\n",
    "        embedding_to_from = tf.keras.layers.Lambda(lambda x: x, name=\"umap\")(\n",
    "            embedding_to_from\n",
    "        )\n",
    "        outputs[\"umap\"] = embedding_to_from\n",
    "\n",
    "        # create model\n",
    "        self.parametric_model = tf.keras.Model(inputs=inputs, outputs=outputs,)\n",
    "\n",
    "    def _compile_model(self):\n",
    "        \"\"\"\n",
    "        Compiles \n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        loss_weights = {}\n",
    "\n",
    "        umap_loss_fn = umap_loss(\n",
    "            self.batch_size,\n",
    "            self.negative_sample_rate,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            self.edge_weight,\n",
    "            self.parametric_embedding,\n",
    "        )\n",
    "        losses[\"umap\"] = umap_loss_fn\n",
    "        loss_weights[\"umap\"] = 1.0\n",
    "\n",
    "        if self.parametric_reconstruction:\n",
    "            losses[\"reconstruction\"] = tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True\n",
    "            )\n",
    "            loss_weights[\"reconstruction\"] = 1.0\n",
    "\n",
    "        self.parametric_model.compile(\n",
    "            optimizer=self.optimizer, loss=losses, loss_weights=loss_weights,\n",
    "        )\n",
    "\n",
    "    def _fit_embed_data(self, X, n_epochs, init, random_state):\n",
    "\n",
    "        # get dimensionality of dataset\n",
    "        if self.dims is None:\n",
    "            self.dims = [np.shape(X)[-1]]\n",
    "        else:\n",
    "            # reshape data for network\n",
    "            if len(self.dims) > 1:\n",
    "                X = np.reshape(X, [len(X)] + list(self.dims))\n",
    "\n",
    "        # get dataset of edges\n",
    "        (\n",
    "            edge_dataset,\n",
    "            self.batch_size,\n",
    "            n_edges,\n",
    "            head,\n",
    "            tail,\n",
    "            self.edge_weight,\n",
    "        ) = construct_edge_dataset(\n",
    "            X,\n",
    "            self.graph_,\n",
    "            self.n_epochs,\n",
    "            self.batch_size,\n",
    "            self.parametric_embedding,\n",
    "            self.parametric_reconstruction,\n",
    "        )\n",
    "        self.head = tf.constant(tf.expand_dims(head, 0))\n",
    "        self.tail = tf.constant(tf.expand_dims(tail, 0))\n",
    "\n",
    "        a, b = next(iter(edge_dataset))\n",
    "        # breakme\n",
    "\n",
    "        if self.parametric_embedding:\n",
    "            init_embedding = None\n",
    "        else:\n",
    "            init_embedding = init_embedding_from_graph(\n",
    "                X,\n",
    "                self.graph_,\n",
    "                self.n_components,\n",
    "                self.random_state,\n",
    "                self.metric,\n",
    "                self._metric_kwds,\n",
    "                init=\"spectral\",\n",
    "            )\n",
    "\n",
    "        # create encoder and decoder model\n",
    "        n_data = len(X)\n",
    "        self.encoder, self.decoder = prepare_networks(\n",
    "            self.encoder,\n",
    "            self.decoder,\n",
    "            self.n_components,\n",
    "            self.dims,\n",
    "            n_data,\n",
    "            self.parametric_embedding,\n",
    "            self.parametric_reconstruction,\n",
    "            init_embedding,\n",
    "        )\n",
    "\n",
    "        # create the model\n",
    "        self._define_model()\n",
    "        self._compile_model()\n",
    "\n",
    "        # report every loss_report_frequency subdivision of an epochs\n",
    "        if self.parametric_embedding:\n",
    "            steps_per_epoch = int(\n",
    "                n_edges / self.batch_size / self.loss_report_frequency\n",
    "            )\n",
    "        else:\n",
    "            # all edges are trained simultaneously with nonparametric, so this is arbitrary\n",
    "            steps_per_epoch = 100\n",
    "\n",
    "        # Validation dataset for reconstruction\n",
    "        if (\n",
    "            self.parametric_reconstruction\n",
    "            and self.reconstruction_validation is not None\n",
    "        ):\n",
    "\n",
    "            # reshape data for network\n",
    "            if len(self.dims) > 1:\n",
    "                self.reconstruction_validation = np.reshape(\n",
    "                    self.reconstruction_validation,\n",
    "                    [len(self.reconstruction_validation)] + list(self.dims),\n",
    "                )\n",
    "\n",
    "            validation_data = (\n",
    "                (\n",
    "                    self.reconstruction_validation,\n",
    "                    tf.zeros_like(self.reconstruction_validation),\n",
    "                ),\n",
    "                {\"reconstruction\": self.reconstruction_validation},\n",
    "            )\n",
    "        else:\n",
    "            validation_data = None\n",
    "\n",
    "        # create embedding\n",
    "        history = self.parametric_model.fit(\n",
    "            edge_dataset,\n",
    "            epochs=self.loss_report_frequency * self.n_training_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            max_queue_size=100,\n",
    "            validation_data=validation_data,\n",
    "            **self.keras_fit_kwargs\n",
    "        )\n",
    "        # save loss history dictionary\n",
    "        self._history = history.history\n",
    "\n",
    "        # get the final embedding\n",
    "        if self.parametric_embedding:\n",
    "            embedding = self.encoder.predict(X, verbose=self.verbose)\n",
    "        else:\n",
    "            embedding = self.encoder.trainable_variables[0].numpy()\n",
    "\n",
    "        return embedding, {}\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # this function supports pickling, making sure that objects can be pickled\n",
    "        return dict((k, v) for (k, v) in self.__dict__.items() if should_pickle(k, v))\n",
    "\n",
    "    def save(self, save_location, verbose=True):\n",
    "\n",
    "        # save encoder\n",
    "        if self.encoder is not None:\n",
    "            encoder_output = os.path.join(save_location, \"encoder\")\n",
    "            self.encoder.save(encoder_output)\n",
    "            if verbose:\n",
    "                print(\"Keras encoder model saved to {}\".format(encoder_output))\n",
    "\n",
    "        # save decoder\n",
    "        if self.decoder is not None:\n",
    "            decoder_output = os.path.join(save_location, \"decoder\")\n",
    "            self.decoder.save(decoder_output)\n",
    "            print(\"Keras decoder model saved to {}\".format(decoder_output))\n",
    "\n",
    "        # save parametric_model\n",
    "        if self.parametric_model is not None:\n",
    "            parametric_model_output = os.path.join(save_location, \"parametric_model\")\n",
    "            self.parametric_model.save(parametric_model_output)\n",
    "            print(\"Keras full model saved to {}\".format(parametric_model_output))\n",
    "\n",
    "        # save model.pkl (ignoring unpickleable warnings)\n",
    "        with catch_warnings():\n",
    "            filterwarnings(\"ignore\")\n",
    "            model_output = os.path.join(save_location, \"model.pkl\")\n",
    "            with open(model_output, \"wb\") as output:\n",
    "                pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "            if verbose:\n",
    "                print(\"Pickle of parametricUMAP model saved to {}\".format(model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:41.761364Z",
     "start_time": "2020-08-17T00:14:41.730563Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Conv2D, LeakyReLU, AvgPool2D, UpSampling2D, Cropping2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.429849Z",
     "start_time": "2020-08-17T00:14:41.762524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 13, 13, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 128)         36992     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,349,826\n",
      "Trainable params: 1,349,826\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dims = (28,28, 1)\n",
    "n_components = 2\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=32, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.469909Z",
     "start_time": "2020-08-17T00:14:42.431541Z"
    }
   },
   "outputs": [],
   "source": [
    "X_valid_flat = X_valid.reshape((len(X_valid), np.product(np.shape(X_valid)[1:])))\n",
    "X_train_flat = X_train.reshape((len(X_train), np.product(np.shape(X_train)[1:])))\n",
    "#X_train_flat = X_train_flat[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.504752Z",
     "start_time": "2020-08-17T00:14:42.472341Z"
    }
   },
   "outputs": [],
   "source": [
    "keras_callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', min_delta=10**-3, patience=10, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=False\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:55.454542Z",
     "start_time": "2020-08-17T00:14:55.410313Z"
    }
   },
   "outputs": [],
   "source": [
    "model = parametricUMAP(\n",
    "    #parametric_embedding=True,\n",
    "    #batch_size = 1000,\n",
    "    #parametric_reconstruction=True,\n",
    "    #loss_report_frequency=10,\n",
    "    #n_training_epochs=1000,\n",
    "    verbose=True,\n",
    "    #autoencoder_loss = True,\n",
    "    #reconstruction_validation = X_valid_flat,\n",
    "    #encoder = encoder,\n",
    "    #decoder = decoder,\n",
    "    #dims = dims,\n",
    "    #keras_callbacks =keras_callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:15:55.576269Z",
     "start_time": "2020-08-17T00:14:55.945963Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parametricUMAP(optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7f16cf549b00>)\n",
      "Construct fuzzy simplicial set\n",
      "Sun Aug 16 17:14:56 2020 Finding Nearest Neighbors\n",
      "Sun Aug 16 17:14:56 2020 Building RP forest with 16 trees\n",
      "Sun Aug 16 17:14:57 2020 parallel NN descent for 16 iterations\n",
      "\t 0  /  16\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\t 4  /  16\n",
      "Sun Aug 16 17:15:07 2020 Finished Nearest Neighbor Search\n",
      "Sun Aug 16 17:15:09 2020 Construct embedding\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 12s 19ms/step - loss: 0.2416\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 12s 19ms/step - loss: 0.1868\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 11s 19ms/step - loss: 0.1656\n",
      "Epoch 4/10\n",
      "163/601 [=======>......................] - ETA: 8s - loss: 0.1583"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-ba2465c7f7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/cube/tsainbur/Projects/github_repos/umap/umap/umap_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   2320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m         self.embedding_, aux_data = self._fit_embed_data(\n\u001b[0;32m-> 2322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# JH why raw data?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2323\u001b[0m         )\n\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-499652a05bdc>\u001b[0m in \u001b[0;36m_fit_embed_data\u001b[0;34m(self, X, n_epochs, init, random_state)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_fit_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         )\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# save loss history dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/mnt/cube/tsainbur/conda_envs/tpy3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embedding = model.fit(X_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:15:55.578773Z",
     "start_time": "2020-08-17T00:14:56.099Z"
    }
   },
   "outputs": [],
   "source": [
    "model.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.823437Z",
     "start_time": "2020-08-17T00:14:32.273Z"
    }
   },
   "outputs": [],
   "source": [
    "type(model.graph_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.823976Z",
     "start_time": "2020-08-17T00:14:32.275Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = model.encoder.predict(X_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.824442Z",
     "start_time": "2020-08-17T00:14:32.276Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.825270Z",
     "start_time": "2020-08-17T00:14:32.278Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots( figsize=(8, 8))\n",
    "sc = ax.scatter(\n",
    "    embedding[:, 0],\n",
    "    embedding[:, 1],\n",
    "    c=Y_train.astype(int),\n",
    "    cmap=\"tab10\",\n",
    "    s=0.1,\n",
    "    alpha=0.5,\n",
    "    rasterized=True,\n",
    ")\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"UMAP in Tensorflow embedding\", fontsize=20)\n",
    "plt.colorbar(sc, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.825968Z",
     "start_time": "2020-08-17T00:14:32.279Z"
    }
   },
   "outputs": [],
   "source": [
    "model._history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.826658Z",
     "start_time": "2020-08-17T00:14:32.281Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(model._history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.827346Z",
     "start_time": "2020-08-17T00:14:32.282Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.828048Z",
     "start_time": "2020-08-17T00:14:32.284Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('/tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.828715Z",
     "start_time": "2020-08-17T00:14:32.285Z"
    }
   },
   "outputs": [],
   "source": [
    "! ls '/tmp/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.829401Z",
     "start_time": "2020-08-17T00:14:32.286Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_parametric_umap_model('/tmp/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.830159Z",
     "start_time": "2020-08-17T00:14:32.288Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.830817Z",
     "start_time": "2020-08-17T00:14:32.289Z"
    }
   },
   "outputs": [],
   "source": [
    "nex = 10\n",
    "x = X_train[:nex]\n",
    "x_recon = model.decoder.predict(model.encoder.predict(x))\n",
    "fig, axs = plt.subplots(ncols=10, nrows = 2, figsize=(nex, 2))\n",
    "for i in range(nex):\n",
    "    axs[0, i].matshow(np.squeeze(x[i]))\n",
    "    axs[1, i].matshow(tf.nn.sigmoid(np.squeeze(x_recon[i].reshape(dims))))\n",
    "for ax in axs.flatten():\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.831505Z",
     "start_time": "2020-08-17T00:14:32.290Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), activation=\"relu\"\n",
    "    ),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=n_components),\n",
    "])\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.832236Z",
     "start_time": "2020-08-17T00:14:32.292Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(n_components)),\n",
    "    tf.keras.layers.Dense(units=512, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=7 * 7 * 256, activation=\"relu\"),\n",
    "    tf.keras.layers.Reshape(target_shape=(7, 7, 256)),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=128, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=64, kernel_size=3, strides=(2, 2), padding=\"SAME\", activation=\"relu\"\n",
    "    ),\n",
    "    tf.keras.layers.Conv2DTranspose(\n",
    "        filters=1, kernel_size=3, strides=(1, 1), padding=\"SAME\", activation=\"sigmoid\"\n",
    "    )\n",
    "])\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:14:42.833470Z",
     "start_time": "2020-08-17T00:14:32.293Z"
    }
   },
   "outputs": [],
   "source": [
    "model.encoder.predict(X_valid, batch_size = model.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

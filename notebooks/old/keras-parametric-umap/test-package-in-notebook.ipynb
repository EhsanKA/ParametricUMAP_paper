{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:29.552553Z",
     "start_time": "2020-08-17T00:17:29.535976Z"
    }
   },
   "outputs": [],
   "source": [
    "# reload packages\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:29.565713Z",
     "start_time": "2020-08-17T00:17:29.553971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:33.351002Z",
     "start_time": "2020-08-17T00:17:29.567396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(gpu_devices)>0:\n",
    "    tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
    "print(gpu_devices)\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:34.300814Z",
     "start_time": "2020-08-17T00:17:33.352634Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tfumap.load_datasets import load_MNIST, mask_labels\n",
    "X_train, X_test, X_valid, Y_train, Y_test, Y_valid = load_MNIST(flatten=False)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:38.668743Z",
     "start_time": "2020-08-17T00:17:34.302777Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from umap import UMAP\n",
    "from warnings import warn, catch_warnings, filterwarnings\n",
    "from umap.umap_ import make_epochs_per_sample\n",
    "from numba import TypingError\n",
    "import os\n",
    "from umap.spectral import spectral_layout\n",
    "from sklearn.utils import check_random_state\n",
    "import codecs, pickle\n",
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "\n",
    "class parametricUMAP(UMAP):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer=None,\n",
    "        batch_size=None,\n",
    "        dims=None,\n",
    "        encoder=None,\n",
    "        decoder=None,\n",
    "        parametric_embedding=True,\n",
    "        parametric_reconstruction=False,\n",
    "        autoencoder_loss=False,\n",
    "        reconstruction_validation=None,\n",
    "        loss_report_frequency=10,\n",
    "        n_training_epochs=1,\n",
    "        keras_fit_kwargs={},\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parametric UMAP subclassing UMAP-learn, based on keras/tensorflow.\n",
    "        There is also a non-parametric implementation contained within to compare \n",
    "        with the base non-parametric implementation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : tf.keras.optimizers, optional\n",
    "            The tensorflow optimizer used for embedding, by default None\n",
    "        batch_size : int, optional\n",
    "            size of batch used for batch training, by default None\n",
    "        dims :  tuple, optional\n",
    "            dimensionality of data, if not flat (e.g. (32x32x3 images for ConvNet), by default None\n",
    "        encoder : tf.keras.Sequential, optional\n",
    "            The encoder Keras network\n",
    "        decoder : tf.keras.Sequential, optional\n",
    "            the decoder Keras network\n",
    "        parametric_embedding : bool, optional\n",
    "            Whether the embedder is parametric or non-parametric, by default True\n",
    "        parametric_reconstruction : bool, optional\n",
    "            Whether the decoder is parametric or non-parametric, by default False\n",
    "        autoencoder_loss : bool, optional\n",
    "            [description], by default False\n",
    "        reconstruction_validation : array, optional\n",
    "            validation X data for reconstruction loss, by default None\n",
    "        loss_report_frequency : int, optional\n",
    "            how many times per epoch to report loss, by default 1\n",
    "        n_training_epochs : int, optional\n",
    "            [description], by default 1\n",
    "        keras_fit_kwargs : dict, optional\n",
    "            additional arguments for model.fit (like callbacks), by default {}\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        # add to network\n",
    "        self.dims = dims  # if this is an image, we should reshape for network\n",
    "        self.encoder = encoder  # neural network used for embedding\n",
    "        self.decoder = decoder  # neural network used for decoding\n",
    "        self.parametric_embedding = (\n",
    "            parametric_embedding  # nonparametric vs parametric embedding\n",
    "        )\n",
    "        self.parametric_reconstruction = parametric_reconstruction\n",
    "        self.autoencoder_loss = autoencoder_loss\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_report_frequency = (\n",
    "            loss_report_frequency  # how many times per epoch to report loss in keras\n",
    "        )\n",
    "        self.reconstruction_validation = (\n",
    "            reconstruction_validation  # holdout data for reconstruction acc\n",
    "        )\n",
    "        self.keras_fit_kwargs = keras_fit_kwargs  # arguments for model.fit\n",
    "        self.parametric_model = None\n",
    "\n",
    "        # how many epochs to train for (different than n_epochs which is specific to each sample)\n",
    "        self.n_training_epochs = n_training_epochs\n",
    "        # set optimizer\n",
    "        if optimizer is None:\n",
    "            if parametric_embedding:\n",
    "                # Adam is better for parametric_embedding\n",
    "                self.optimizer = tf.keras.optimizers.Adam(1e-3)\n",
    "            else:\n",
    "                # Larger learning rate can be used for embedding\n",
    "                self.optimizer = tf.keras.optimizers.Adam(1e-1)\n",
    "        else:\n",
    "            self.optimizer = optimizer\n",
    "\n",
    "        if parametric_reconstruction and not parametric_embedding:\n",
    "            warn(\n",
    "                \"Parametric decoding is not implemented with nonparametric \\\n",
    "            embedding. Turning off parametric decoding\"\n",
    "            )\n",
    "            self.parametric_reconstruction = False\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X into the existing embedded space and return that\n",
    "        transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_features)\n",
    "            New data to be transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_components)\n",
    "            Embedding of the new data in low-dimensional space.\n",
    "        \"\"\"\n",
    "        if self.parametric_embedding:\n",
    "            return self.encoder.predict(\n",
    "                X, batch_size=self.batch_size, verbose=self.verbose\n",
    "            )\n",
    "        else:\n",
    "            warn(\n",
    "                \"Embedding new data is not supported by parametricUMAP. \\\n",
    "                Using original embedder.\"\n",
    "            )\n",
    "            return super().transform(X)\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        \"\"\"\"Transform X in the existing embedded space back into the input\n",
    "        data space and return that transformed output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array, shape (n_samples, n_components)\n",
    "            New points to be inverse transformed.\n",
    "        Returns\n",
    "        -------\n",
    "        X_new : array, shape (n_samples, n_features)\n",
    "            Generated data points new data in data space.\n",
    "        \"\"\"\n",
    "        if self.parametric_reconstruction:\n",
    "            return self.decoder.predict(\n",
    "                X, batch_size=self.batch_size, verbose=self.verbose\n",
    "            )\n",
    "        else:\n",
    "            return super().inverse_transform(X)\n",
    "\n",
    "    def _define_model(self):\n",
    "        \"\"\" Define the model in keras\n",
    "        \"\"\"\n",
    "\n",
    "        # network outputs\n",
    "        outputs = {}\n",
    "\n",
    "        # inputs\n",
    "        if self.parametric_embedding:\n",
    "            to_x = tf.keras.layers.Input(shape=self.dims, name=\"to_x\")\n",
    "            from_x = tf.keras.layers.Input(shape=self.dims, name=\"from_x\")\n",
    "            inputs = [to_x, from_x]\n",
    "\n",
    "            # parametric embedding\n",
    "            embedding_to = self.encoder(to_x)\n",
    "            embedding_from = self.encoder(from_x)\n",
    "\n",
    "            if self.parametric_reconstruction:\n",
    "                # parametric reconstruction\n",
    "                if self.autoencoder_loss:\n",
    "                    embedding_to_recon = self.decoder(embedding_to)\n",
    "                else:\n",
    "                    # stop gradient of reconstruction loss before it reaches the encoder\n",
    "                    embedding_to_recon = self.decoder(tf.stop_gradient(embedding_to))\n",
    "\n",
    "                embedding_to_recon = tf.keras.layers.Lambda(\n",
    "                    lambda x: x, name=\"reconstruction\"\n",
    "                )(embedding_to_recon)\n",
    "\n",
    "                outputs[\"reconstruction\"] = embedding_to_recon\n",
    "\n",
    "        else:\n",
    "            # this is the sham input (its just a 0) to make keras think there is input data\n",
    "            batch_sample = tf.keras.layers.Input(\n",
    "                shape=(1), dtype=tf.int32, name=\"batch_sample\"\n",
    "            )\n",
    "\n",
    "            # gather all of the edges (so keras model is happy)\n",
    "            to_x = tf.squeeze(tf.gather(self.head, batch_sample[0]))\n",
    "            from_x = tf.squeeze(tf.gather(self.tail, batch_sample[0]))\n",
    "\n",
    "            # grab relevant embeddings\n",
    "            embedding_to = self.encoder(to_x)[:, -1, :]\n",
    "            embedding_from = self.encoder(from_x)[:, -1, :]\n",
    "\n",
    "            inputs = [batch_sample]\n",
    "\n",
    "        # concatenate to/from projections for loss computation\n",
    "        embedding_to_from = tf.concat([embedding_to, embedding_from], axis=1)\n",
    "        embedding_to_from = tf.keras.layers.Lambda(lambda x: x, name=\"umap\")(\n",
    "            embedding_to_from\n",
    "        )\n",
    "        outputs[\"umap\"] = embedding_to_from\n",
    "\n",
    "        # create model\n",
    "        self.parametric_model = tf.keras.Model(inputs=inputs, outputs=outputs,)\n",
    "\n",
    "    def _compile_model(self):\n",
    "        \"\"\"\n",
    "        Compiles \n",
    "        \"\"\"\n",
    "        losses = {}\n",
    "        loss_weights = {}\n",
    "\n",
    "        umap_loss_fn = umap_loss(\n",
    "            self.batch_size,\n",
    "            self.negative_sample_rate,\n",
    "            self._a,\n",
    "            self._b,\n",
    "            self.edge_weight,\n",
    "            self.parametric_embedding,\n",
    "        )\n",
    "        losses[\"umap\"] = umap_loss_fn\n",
    "        loss_weights[\"umap\"] = 1.0\n",
    "\n",
    "        if self.parametric_reconstruction:\n",
    "            losses[\"reconstruction\"] = tf.keras.losses.BinaryCrossentropy(\n",
    "                from_logits=True\n",
    "            )\n",
    "            loss_weights[\"reconstruction\"] = 1.0\n",
    "\n",
    "        self.parametric_model.compile(\n",
    "            optimizer=self.optimizer, loss=losses, loss_weights=loss_weights,\n",
    "        )\n",
    "\n",
    "    def _fit_embed_data(self, X, n_epochs, init, random_state):\n",
    "\n",
    "        # get dimensionality of dataset\n",
    "        if self.dims is None:\n",
    "            self.dims = [np.shape(X)[-1]]\n",
    "        else:\n",
    "            # reshape data for network\n",
    "            if len(self.dims) > 1:\n",
    "                X = np.reshape(X, [len(X)] + list(self.dims))\n",
    "\n",
    "        # get dataset of edges\n",
    "        (\n",
    "            edge_dataset,\n",
    "            self.batch_size,\n",
    "            n_edges,\n",
    "            head,\n",
    "            tail,\n",
    "            self.edge_weight,\n",
    "        ) = construct_edge_dataset(\n",
    "            X,\n",
    "            self.graph_,\n",
    "            self.n_epochs,\n",
    "            self.batch_size,\n",
    "            self.parametric_embedding,\n",
    "            self.parametric_reconstruction,\n",
    "        )\n",
    "        self.head = tf.constant(tf.expand_dims(head, 0))\n",
    "        self.tail = tf.constant(tf.expand_dims(tail, 0))\n",
    "\n",
    "        a, b = next(iter(edge_dataset))\n",
    "        # breakme\n",
    "\n",
    "        if self.parametric_embedding:\n",
    "            init_embedding = None\n",
    "        else:\n",
    "            init_embedding = init_embedding_from_graph(\n",
    "                X,\n",
    "                self.graph_,\n",
    "                self.n_components,\n",
    "                self.random_state,\n",
    "                self.metric,\n",
    "                self._metric_kwds,\n",
    "                init=\"spectral\",\n",
    "            )\n",
    "\n",
    "        # create encoder and decoder model\n",
    "        n_data = len(X)\n",
    "        self.encoder, self.decoder = prepare_networks(\n",
    "            self.encoder,\n",
    "            self.decoder,\n",
    "            self.n_components,\n",
    "            self.dims,\n",
    "            n_data,\n",
    "            self.parametric_embedding,\n",
    "            self.parametric_reconstruction,\n",
    "            init_embedding,\n",
    "        )\n",
    "\n",
    "        # create the model\n",
    "        self._define_model()\n",
    "        self._compile_model()\n",
    "\n",
    "        # report every loss_report_frequency subdivision of an epochs\n",
    "        if self.parametric_embedding:\n",
    "            steps_per_epoch = int(\n",
    "                n_edges / self.batch_size / self.loss_report_frequency\n",
    "            )\n",
    "        else:\n",
    "            # all edges are trained simultaneously with nonparametric, so this is arbitrary\n",
    "            steps_per_epoch = 100\n",
    "\n",
    "        # Validation dataset for reconstruction\n",
    "        if (\n",
    "            self.parametric_reconstruction\n",
    "            and self.reconstruction_validation is not None\n",
    "        ):\n",
    "\n",
    "            # reshape data for network\n",
    "            if len(self.dims) > 1:\n",
    "                self.reconstruction_validation = np.reshape(\n",
    "                    self.reconstruction_validation,\n",
    "                    [len(self.reconstruction_validation)] + list(self.dims),\n",
    "                )\n",
    "\n",
    "            validation_data = (\n",
    "                (\n",
    "                    self.reconstruction_validation,\n",
    "                    tf.zeros_like(self.reconstruction_validation),\n",
    "                ),\n",
    "                {\"reconstruction\": self.reconstruction_validation},\n",
    "            )\n",
    "        else:\n",
    "            validation_data = None\n",
    "\n",
    "        # create embedding\n",
    "        history = self.parametric_model.fit(\n",
    "            edge_dataset,\n",
    "            epochs=self.loss_report_frequency * self.n_training_epochs,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            max_queue_size=100,\n",
    "            validation_data=validation_data,\n",
    "            **self.keras_fit_kwargs\n",
    "        )\n",
    "        # save loss history dictionary\n",
    "        self._history = history.history\n",
    "\n",
    "        # get the final embedding\n",
    "        if self.parametric_embedding:\n",
    "            embedding = self.encoder.predict(X, verbose=self.verbose)\n",
    "        else:\n",
    "            embedding = self.encoder.trainable_variables[0].numpy()\n",
    "\n",
    "        return embedding, {}\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # this function supports pickling, making sure that objects can be pickled\n",
    "        return dict((k, v) for (k, v) in self.__dict__.items() if should_pickle(k, v))\n",
    "\n",
    "    def save(self, save_location, verbose=True):\n",
    "\n",
    "        # save encoder\n",
    "        if self.encoder is not None:\n",
    "            encoder_output = os.path.join(save_location, \"encoder\")\n",
    "            self.encoder.save(encoder_output)\n",
    "            if verbose:\n",
    "                print(\"Keras encoder model saved to {}\".format(encoder_output))\n",
    "\n",
    "        # save decoder\n",
    "        if self.decoder is not None:\n",
    "            decoder_output = os.path.join(save_location, \"decoder\")\n",
    "            self.decoder.save(decoder_output)\n",
    "            print(\"Keras decoder model saved to {}\".format(decoder_output))\n",
    "\n",
    "        # save parametric_model\n",
    "        if self.parametric_model is not None:\n",
    "            parametric_model_output = os.path.join(save_location, \"parametric_model\")\n",
    "            self.parametric_model.save(parametric_model_output)\n",
    "            print(\"Keras full model saved to {}\".format(parametric_model_output))\n",
    "\n",
    "        # save model.pkl (ignoring unpickleable warnings)\n",
    "        with catch_warnings():\n",
    "            filterwarnings(\"ignore\")\n",
    "            model_output = os.path.join(save_location, \"model.pkl\")\n",
    "            with open(model_output, \"wb\") as output:\n",
    "                pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "            if verbose:\n",
    "                print(\"Pickle of parametricUMAP model saved to {}\".format(model_output))\n",
    "\n",
    "\n",
    "def get_graph_elements(graph_, n_epochs):\n",
    "    \"\"\"\n",
    "    gets elements of graphs, weights, and number of epochs per edge\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph_ : scipy.sparse.csr.csr_matrix\n",
    "        umap graph of probabilities\n",
    "    n_epochs : int\n",
    "        maximum number of epochs per edge\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    graph scipy.sparse.csr.csr_matrix\n",
    "        umap graph\n",
    "    epochs_per_sample np.array\n",
    "        number of epochs to train each sample for\n",
    "    head np.array\n",
    "        edge head\n",
    "    tail np.array\n",
    "        edge tail\n",
    "    weight np.array\n",
    "        edge weight\n",
    "    n_vertices int\n",
    "        number of verticies in graph\n",
    "    \"\"\"\n",
    "    ### should we remove redundancies () here??\n",
    "    # graph_ = remove_redundant_edges(graph_)\n",
    "\n",
    "    graph = graph_.tocoo()\n",
    "    # eliminate duplicate entries by summing them together\n",
    "    graph.sum_duplicates()\n",
    "    # number of vertices in dataset\n",
    "    n_vertices = graph.shape[1]\n",
    "    # get the number of epochs based on the size of the dataset\n",
    "    if n_epochs is None:\n",
    "        # For smaller datasets we can use more epochs\n",
    "        if graph.shape[0] <= 10000:\n",
    "            n_epochs = 500\n",
    "        else:\n",
    "            n_epochs = 200\n",
    "    # remove elements with very low probability\n",
    "    graph.data[graph.data < (graph.data.max() / float(n_epochs))] = 0.0\n",
    "    graph.eliminate_zeros()\n",
    "    # get epochs per sample based upon edge probability\n",
    "    epochs_per_sample = make_epochs_per_sample(graph.data, n_epochs)\n",
    "\n",
    "    head = graph.row\n",
    "    tail = graph.col\n",
    "    weight = graph.data\n",
    "\n",
    "    return graph, epochs_per_sample, head, tail, weight, n_vertices\n",
    "\n",
    "\n",
    "def init_embedding_from_graph(\n",
    "    _raw_data, graph, n_components, random_state, metric, _metric_kwds, init=\"spectral\"\n",
    "):\n",
    "    \"\"\" Initialize embedding using graph. This is for direct embeddings. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        init : str, optional\n",
    "            Type of initialization to use. Either random, or spectral, by default \"spectral\"\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embedding : np.array\n",
    "            the initialized embedding\n",
    "        \"\"\"\n",
    "    if random_state is None:\n",
    "        random_state = check_random_state(None)\n",
    "\n",
    "    if isinstance(init, str) and init == \"random\":\n",
    "        embedding = random_state.uniform(\n",
    "            low=-10.0, high=10.0, size=(graph.shape[0], n_components)\n",
    "        ).astype(np.float32)\n",
    "    elif isinstance(init, str) and init == \"spectral\":\n",
    "        # We add a little noise to avoid local minima for optimization to come\n",
    "\n",
    "        initialisation = spectral_layout(\n",
    "            _raw_data,\n",
    "            graph,\n",
    "            n_components,\n",
    "            random_state,\n",
    "            metric=metric,\n",
    "            metric_kwds=_metric_kwds,\n",
    "        )\n",
    "        expansion = 10.0 / np.abs(initialisation).max()\n",
    "        embedding = (initialisation * expansion).astype(\n",
    "            np.float32\n",
    "        ) + random_state.normal(\n",
    "            scale=0.0001, size=[graph.shape[0], n_components]\n",
    "        ).astype(\n",
    "            np.float32\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        init_data = np.array(init)\n",
    "        if len(init_data.shape) == 2:\n",
    "            if np.unique(init_data, axis=0).shape[0] < init_data.shape[0]:\n",
    "                tree = KDTree(init_data)\n",
    "                dist, ind = tree.query(init_data, k=2)\n",
    "                nndist = np.mean(dist[:, 1])\n",
    "                embedding = init_data + random_state.normal(\n",
    "                    scale=0.001 * nndist, size=init_data.shape\n",
    "                ).astype(np.float32)\n",
    "            else:\n",
    "                embedding = init_data\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def convert_distance_to_probability(distances, a=1.0, b=1.0):\n",
    "    \"\"\"\n",
    "     convert distance representation into probability, \n",
    "        as a function of a, b params\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distances : array\n",
    "        euclidean distance between two points in embedding\n",
    "    a : float, optional\n",
    "        parameter based on min_dist, by default 1.0\n",
    "    b : float, optional\n",
    "        parameter based on min_dist, by default 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        probability in embedding space\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + a * distances ** (2 * b))\n",
    "\n",
    "\n",
    "def compute_cross_entropy(\n",
    "    probabilities_graph, probabilities_distance, EPS=1e-4, repulsion_strength=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute cross entropy between low and high probability\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    probabilities_graph : array\n",
    "        high dimensional probabilities\n",
    "    probabilities_distance : array\n",
    "        low dimensional probabilities\n",
    "    EPS : float, optional\n",
    "        offset to to ensure log is taken of a positive number, by default 1e-4\n",
    "    repulsion_strength : float, optional\n",
    "        strength of repulsion between negative samples, by default 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    attraction_term: tf.float32\n",
    "        attraction term for cross entropy loss\n",
    "    repellant_term: tf.float32\n",
    "        repellant term for cross entropy loss\n",
    "    cross_entropy: tf.float32\n",
    "        cross entropy umap loss\n",
    "    \n",
    "    \"\"\"\n",
    "    # cross entropy\n",
    "    attraction_term = -probabilities_graph * tf.math.log(\n",
    "        tf.clip_by_value(probabilities_distance, EPS, 1.0)\n",
    "    )\n",
    "    repellant_term = (\n",
    "        -(1.0 - probabilities_graph)\n",
    "        * tf.math.log(tf.clip_by_value(1.0 - probabilities_distance, EPS, 1.0))\n",
    "        * repulsion_strength\n",
    "    )\n",
    "\n",
    "    # balance the expected losses between atrraction and repel\n",
    "    CE = attraction_term + repellant_term\n",
    "    return attraction_term, repellant_term, CE\n",
    "\n",
    "\n",
    "def umap_loss(\n",
    "    batch_size,\n",
    "    negative_sample_rate,\n",
    "    _a,\n",
    "    _b,\n",
    "    edge_weights,\n",
    "    parametric_embedding,\n",
    "    repulsion_strength=1.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a keras-ccompatible loss function for UMAP loss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        size of mini-batches\n",
    "    negative_sample_rate : int\n",
    "        number of negative samples per positive samples to train on\n",
    "    _a : float\n",
    "        distance parameter in embedding space\n",
    "    _b : float float\n",
    "        distance parameter in embedding space\n",
    "    edge_weights : array\n",
    "        weights of all edges from sparse UMAP graph\n",
    "    parametric_embedding : bool\n",
    "        whether the embeddding is parametric or nonparametric\n",
    "    repulsion_strength : float, optional\n",
    "        strength of repulsion vs attraction for cross-entropy, by default 1.0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : function\n",
    "        loss function that takes in a placeholder (0) and the output of the keras network\n",
    "    \"\"\"\n",
    "\n",
    "    if not parametric_embedding:\n",
    "        # multiply loss by weights for nonparametric\n",
    "        weights_tiled = np.tile(edge_weights, negative_sample_rate + 1)\n",
    "\n",
    "    @tf.function\n",
    "    def loss(placeholder_y, embed_to_from):\n",
    "        # split out to/from\n",
    "        embedding_to, embedding_from = tf.split(\n",
    "            embed_to_from, num_or_size_splits=2, axis=1\n",
    "        )\n",
    "\n",
    "        # get negative samples\n",
    "        embedding_neg_to = tf.repeat(embedding_to, negative_sample_rate, axis=0)\n",
    "        repeat_neg = tf.repeat(embedding_from, negative_sample_rate, axis=0)\n",
    "        embedding_neg_from = tf.gather(\n",
    "            repeat_neg, tf.random.shuffle(tf.range(tf.shape(repeat_neg)[0]))\n",
    "        )\n",
    "\n",
    "        #  distances between samples (and negative samples)\n",
    "        distance_embedding = tf.concat(\n",
    "            [\n",
    "                tf.norm(embedding_to - embedding_from, axis=1),\n",
    "                tf.norm(embedding_neg_to - embedding_neg_from, axis=1),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        # convert probabilities to distances\n",
    "        probabilities_distance = convert_distance_to_probability(\n",
    "            distance_embedding, _a, _b\n",
    "        )\n",
    "\n",
    "        # set true probabilities based on negative sampling\n",
    "        probabilities_graph = tf.concat(\n",
    "            [tf.ones(batch_size), tf.zeros(batch_size * negative_sample_rate)], axis=0,\n",
    "        )\n",
    "\n",
    "        # compute cross entropy\n",
    "        (attraction_loss, repellant_loss, ce_loss) = compute_cross_entropy(\n",
    "            probabilities_graph,\n",
    "            probabilities_distance,\n",
    "            repulsion_strength=repulsion_strength,\n",
    "        )\n",
    "\n",
    "        if not parametric_embedding:\n",
    "            ce_loss = ce_loss * weights_tiled\n",
    "\n",
    "        return tf.reduce_mean(ce_loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def prepare_networks(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    n_components,\n",
    "    dims,\n",
    "    n_data,\n",
    "    parametric_embedding,\n",
    "    parametric_reconstruction,\n",
    "    init_embedding=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a set of keras networks for the encoder and decoder if one has not already\n",
    "    been predefined. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoder : tf.keras.Sequential\n",
    "        The encoder Keras network\n",
    "    decoder : tf.keras.Sequential\n",
    "        the decoder Keras network\n",
    "    n_components : int\n",
    "        the dimensionality of the latent space\n",
    "    dims : tuple of shape (dim1, dim2, dim3...)\n",
    "        dimensionality of data\n",
    "    n_data : number of elements in dataset\n",
    "        # of elements in training dataset\n",
    "    parametric_embedding : bool\n",
    "        Whether the embedder is parametric or non-parametric\n",
    "    parametric_reconstruction : bool\n",
    "        Whether the decoder is parametric or non-parametric\n",
    "    init_embedding : array (optional, default None)\n",
    "        The initial embedding, for nonparametric embeddings\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoder: tf.keras.Sequential\n",
    "        encoder keras network\n",
    "    decoder: tf.keras.Sequential\n",
    "        decoder keras network\n",
    "    \"\"\"\n",
    "\n",
    "    if parametric_embedding:\n",
    "        if encoder is None:\n",
    "            encoder = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.InputLayer(input_shape=dims),\n",
    "                    tf.keras.layers.Flatten(),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=n_components, name=\"z\"),\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            n_data, n_components, input_length=1\n",
    "        )\n",
    "        embedding_layer.build(input_shape=(1,))\n",
    "        embedding_layer.set_weights([init_embedding])\n",
    "        encoder = tf.keras.Sequential([embedding_layer])\n",
    "\n",
    "    if decoder is None:\n",
    "        if parametric_reconstruction:\n",
    "            decoder = tf.keras.Sequential(\n",
    "                [\n",
    "                    tf.keras.layers.InputLayer(input_shape=n_components),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(units=100, activation=\"relu\"),\n",
    "                    tf.keras.layers.Dense(\n",
    "                        units=np.product(dims), name=\"recon\", activation=None\n",
    "                    ),\n",
    "                    tf.keras.layers.Reshape(dims),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "def construct_edge_dataset(\n",
    "    X, graph_, n_epochs, batch_size, parametric_embedding, parametric_reconstruction,\n",
    "):\n",
    "    \"\"\"\n",
    "    Construct a tf.data.Dataset of edges, sampled by edge weight.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n_samples, n_features)\n",
    "        New data to be transformed.\n",
    "    graph_ : scipy.sparse.csr.csr_matrix\n",
    "        Generated UMAP graph\n",
    "    n_epochs : int\n",
    "        # of epochs to train each edge\n",
    "    batch_size : int\n",
    "        batch size\n",
    "    parametric_embedding : bool\n",
    "        Whether the embedder is parametric or non-parametric\n",
    "    parametric_reconstruction : bool\n",
    "        Whether the decoder is parametric or non-parametric\n",
    "    \"\"\"\n",
    "\n",
    "    def gather_X(edge_to, edge_from):\n",
    "        edge_to_batch = tf.gather(X, edge_to)\n",
    "        edge_from_batch = tf.gather(X, edge_from)\n",
    "        outputs = {\"umap\": 0}\n",
    "        if parametric_reconstruction:\n",
    "            # add reconstruction to iterator output\n",
    "            # edge_out = tf.concat([edge_to_batch, edge_from_batch], axis=0)\n",
    "            outputs[\"reconstruction\"] = edge_to_batch\n",
    "\n",
    "        return (edge_to_batch, edge_from_batch), outputs\n",
    "\n",
    "    def make_sham_generator():\n",
    "        \"\"\"\n",
    "        The sham generator is used to \n",
    "        \"\"\"\n",
    "\n",
    "        def sham_generator():\n",
    "            while True:\n",
    "                yield tf.zeros(1, dtype=tf.int32), tf.zeros(1, dtype=tf.int32)\n",
    "\n",
    "        return sham_generator\n",
    "\n",
    "    # get data from graph\n",
    "    graph, epochs_per_sample, head, tail, weight, n_vertices = get_graph_elements(\n",
    "        graph_, n_epochs\n",
    "    )\n",
    "\n",
    "    # number of elements per batch for embedding\n",
    "    if batch_size is None:\n",
    "        # batch size can be larger if its just over embeddings\n",
    "        if parametric_embedding:\n",
    "            batch_size = np.min([n_vertices, 1000])\n",
    "        else:\n",
    "            batch_size = len(head)\n",
    "\n",
    "    edges_to_exp, edges_from_exp = (\n",
    "        np.repeat(head, epochs_per_sample.astype(\"int\")),\n",
    "        np.repeat(tail, epochs_per_sample.astype(\"int\")),\n",
    "    )\n",
    "\n",
    "    # shuffle edges\n",
    "    shuffle_mask = np.random.permutation(range(len(edges_to_exp)))\n",
    "    edges_to_exp = edges_to_exp[shuffle_mask]\n",
    "    edges_from_exp = edges_from_exp[shuffle_mask]\n",
    "\n",
    "    # create edge iterator\n",
    "    if parametric_embedding:\n",
    "        edge_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (edges_to_exp, edges_from_exp)\n",
    "        )\n",
    "        edge_dataset = edge_dataset.repeat()\n",
    "        edge_dataset = edge_dataset.shuffle(10000)\n",
    "        edge_dataset = edge_dataset.map(\n",
    "            gather_X, num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    "        )\n",
    "        edge_dataset = edge_dataset.batch(batch_size, drop_remainder=True)\n",
    "        edge_dataset = edge_dataset.prefetch(10)\n",
    "    else:\n",
    "        # nonparametric embedding uses a sham dataset\n",
    "        gen = make_sham_generator()\n",
    "        edge_dataset = tf.data.Dataset.from_generator(gen, (tf.int32, tf.int32))\n",
    "    return edge_dataset, batch_size, len(edges_to_exp), head, tail, weight\n",
    "\n",
    "\n",
    "def should_pickle(key, val):\n",
    "    \"\"\"\n",
    "    Checks if a dictionary item can be pickled\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : try\n",
    "        key for dictionary element\n",
    "    val : None\n",
    "        element of dictionary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    picklable: bool\n",
    "        whether the dictionary item can be pickled\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ## make sure object can be pickled and then re-read\n",
    "        # pickle object\n",
    "        pickled = codecs.encode(pickle.dumps(val), \"base64\").decode()\n",
    "        # unpickle object\n",
    "        unpickled = pickle.loads(codecs.decode(pickled.encode(), \"base64\"))\n",
    "    except (\n",
    "        pickle.PicklingError,\n",
    "        tf.errors.InvalidArgumentError,\n",
    "        TypeError,\n",
    "        tf.errors.InternalError,\n",
    "        OverflowError,\n",
    "        TypingError,\n",
    "    ) as e:\n",
    "        warn(\"Did not pickle {}: {}\".format(key, e))\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:38.707091Z",
     "start_time": "2020-08-17T00:17:38.670289Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_flat = X_train.reshape((X_train.shape[0], -1))#/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:17:38.742237Z",
     "start_time": "2020-08-17T00:17:38.708522Z"
    }
   },
   "outputs": [],
   "source": [
    "embedder = parametricUMAP(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:20:03.647183Z",
     "start_time": "2020-08-17T00:17:38.743532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parametricUMAP(optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd3a252e978>)\n",
      "Construct fuzzy simplicial set\n",
      "Sun Aug 16 17:17:38 2020 Finding Nearest Neighbors\n",
      "Sun Aug 16 17:17:38 2020 Building RP forest with 16 trees\n",
      "Sun Aug 16 17:17:40 2020 parallel NN descent for 16 iterations\n",
      "\t 0  /  16\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\t 4  /  16\n",
      "Sun Aug 16 17:17:51 2020 Finished Nearest Neighbor Search\n",
      "Sun Aug 16 17:17:53 2020 Construct embedding\n",
      "Epoch 1/10\n",
      "601/601 [==============================] - 13s 21ms/step - loss: 0.2340\n",
      "Epoch 2/10\n",
      "601/601 [==============================] - 12s 20ms/step - loss: 0.1814\n",
      "Epoch 3/10\n",
      "601/601 [==============================] - 12s 21ms/step - loss: 0.1619\n",
      "Epoch 4/10\n",
      "601/601 [==============================] - 12s 21ms/step - loss: 0.1527\n",
      "Epoch 5/10\n",
      "601/601 [==============================] - 12s 19ms/step - loss: 0.1465\n",
      "Epoch 6/10\n",
      "601/601 [==============================] - 12s 20ms/step - loss: 0.1418\n",
      "Epoch 7/10\n",
      "601/601 [==============================] - 12s 19ms/step - loss: 0.1389\n",
      "Epoch 8/10\n",
      "601/601 [==============================] - 12s 20ms/step - loss: 0.1364\n",
      "Epoch 9/10\n",
      "601/601 [==============================] - 12s 19ms/step - loss: 0.1346\n",
      "Epoch 10/10\n",
      "601/601 [==============================] - 12s 19ms/step - loss: 0.1329\n",
      "1563/1563 [==============================] - 1s 686us/step\n",
      "Sun Aug 16 17:20:03 2020 Finished embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parametricUMAP(batch_size=1000, dims=[784],\n",
       "               encoder=<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fd3703750f0>,\n",
       "               optimizer=<tensorflow.python.keras.optimizer_v2.adam.Adam object at 0x7fd3a252e978>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.fit(X_train_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-17T00:20:04.183604Z",
     "start_time": "2020-08-17T00:20:03.649443Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
